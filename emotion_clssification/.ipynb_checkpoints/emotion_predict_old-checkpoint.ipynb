{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()\n",
    "\n",
    "# feel free to use this import\n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "vocab_to_int = {word:ii+1 for ii,word in enumerate(set(words))}\n",
    "#counts = Counter(words)\n",
    "#vocab_sort = sorted(counts,key= counts.get,reverse=True)\n",
    "#vocab_to_int = {word:ii for ii,word in enumerate(vocab_sort,1)}\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    review_ints = []\n",
    "    for word in review.split():\n",
    "        word_to_int = vocab_to_int[word]\n",
    "        review_ints.append(word_to_int)\n",
    "    reviews_ints.append(review_ints)\n",
    "\n",
    "\n",
    "label_split = labels.split('\\n')\n",
    "encoded_labels = []\n",
    "for label in label_split:\n",
    "    if label == 'positive':\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    encoded_labels.append(label)\n",
    "\n",
    "\n",
    "for i in range(len(reviews_ints)):\n",
    "    if len(reviews_ints[i]) == 0:\n",
    "        reviews_ints.pop(i)\n",
    "        encoded_labels.pop(i)\n",
    "\n",
    "\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's\n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    ## implement function\n",
    "    features = []\n",
    "    for review in reviews_ints:\n",
    "        if len(review) < seq_length:\n",
    "            zero_list = np.zeros((seq_length - len(review))).tolist()\n",
    "            review = zero_list + review\n",
    "        else:\n",
    "            review = review[:seq_length]\n",
    "        features.append(review)\n",
    "    features = np.array(features).astype(np.int)\n",
    "    features = np.array(features).reshape((len(reviews_ints), seq_length))\n",
    "\n",
    "    return features\n",
    "\n",
    "seq_length = 400\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "split_frac = 0.8\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "train_x,val_test_x = features[:int(len(features)*split_frac)],features[int(len(features)*split_frac):]\n",
    "val_x,test_x = val_test_x[:len(val_test_x)//2],val_test_x[len(val_test_x)//2:]\n",
    "\n",
    "train_y,val_test_y = encoded_labels[:int(len(encoded_labels)*split_frac)],encoded_labels[int(len(encoded_labels)*split_frac):]\n",
    "val_y,test_y = val_test_y[:len(val_test_y)//2],val_test_y[len(val_test_y)//2:]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "s= torch.cuda.is_available()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # define all layers\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "       # x = torch.LongTensor(x)\n",
    "        embed = self.embed(x)\n",
    "        output, hidden = self.lstm(embed, hidden)\n",
    "        output = output.contiguous().view(-1, self.hidden_dim)\n",
    "        output = self.fc1(output)\n",
    "        sig_out = self.sigmoid(output)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,epochs,train_loader,valid_loader,clip,print_every,lr = 0.001):\n",
    "    # train for some number of epochs\n",
    "    # loss and optimization functions\n",
    "    \n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    counter = 0\n",
    "    loss_min = np.inf\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if (train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                accuracy = []\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if (train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    pre = torch.round(output)\n",
    "                    equal = torch.eq(pre.squeeze(),labels.float().view_as(pre))\n",
    "                    accu = torch.mean(equal.float())\n",
    "\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    accuracy.append(accu.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}...\".format(np.mean(val_losses)),\n",
    "                      'Val Accuracy:{}'.format(np.mean(accuracy)))\n",
    "                if  np.mean(val_losses)<loss_min :\n",
    "                    print('loss decreasing...')\n",
    "                    torch.save(net.state_dict(),'net_emotion_loss_min_old.pth')\n",
    "                    loss_min = np.mean(val_losses)\n",
    "    print('min loss',loss_min)\n",
    "def test(net,test_loader):\n",
    "    # Get test data loss and accuracy\n",
    "    lr = 0.001\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    test_losses = []  # track loss\n",
    "    num_correct = 0\n",
    "\n",
    "    # init hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    class_correct = np.zeros(2)\n",
    "    class_total = np.zeros(2)\n",
    "    classes = ['positive','negative']\n",
    "    # iterate over test data\n",
    "    for inputs, y in test_loader:\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            inputs, y = inputs.cuda(), y.cuda()\n",
    "        # get predicted outputs\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate loss\n",
    "        test_loss = criterion(output.squeeze(), y.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(y.float().view_as(pred))\n",
    "        for i in range(y.shape[0]):\n",
    "            label = y.data[i].item()\n",
    "            class_correct[label] += correct_tensor[i].item()\n",
    "            class_total[label] += 1\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "    # -- stats! -- ##\n",
    "    # avg test loss\n",
    "    for i in range(2):\n",
    "        if class_total[i]>0:\n",
    "            print('Test Accuracy of {}:{:.4f}({}/{})'.format(classes[i],100*class_correct[i]/class_total[i],\n",
    "                 int(np.sum(class_correct[i])),\n",
    "                 int(np.sum(class_total[i]))))\n",
    "        else:\n",
    "            print('Test Accuracy of {}:N/A(no examples)'.format(classes[i]))\n",
    "    print('Test Accuracy(Overall):{:.4f} ({}/{})'.format(100*np.sum(class_correct)/np.sum(class_total),\n",
    "                                                    int(np.sum(class_correct)),\n",
    "                                                    int(np.sum(class_total))))\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "    # accuracy over all test data\n",
    "    test_acc = num_correct / len(test_loader.dataset)\n",
    "    print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "    ''' Prints out whether a give review is predicted to be\n",
    "        positive or negative in sentiment, using a trained model.\n",
    "\n",
    "        params:\n",
    "        net - A trained net\n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "    test_review = test_review.lower()\n",
    "    test_review = ''.join([c for c in test_review if c not in punctuation])\n",
    "    test_split = test_review.split()\n",
    "    review_to_int = [[vocab_to_int[word] for word in test_split]]\n",
    "    feature = pad_features(review_to_int, sequence_length)\n",
    "\n",
    "    feature_tensor = torch.from_numpy(feature)\n",
    "    batch_size = feature_tensor.size(0)\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    if train_on_gpu:\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    output, h = net(feature_tensor, h)\n",
    "    pred = torch.round(output.squeeze())\n",
    "\n",
    "    if pred == 1:\n",
    "        print('It is a positive review')\n",
    "    else:\n",
    "        print('It is a negative review')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embed): Embedding(74073, 256)\n",
      "  (lstm): LSTM(256, 128, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 1/4... Step: 100... Loss: 0.688437... Val Loss: 0.588706... Val Accuracy:0.7020000052452088\n",
      "loss decreasing...\n",
      "Epoch: 1/4... Step: 200... Loss: 0.487893... Val Loss: 0.539477... Val Accuracy:0.7391999995708466\n",
      "loss decreasing...\n",
      "Epoch: 1/4... Step: 300... Loss: 0.978498... Val Loss: 0.778802... Val Accuracy:0.5344000053405762\n",
      "Epoch: 1/4... Step: 400... Loss: 0.416472... Val Loss: 0.394742... Val Accuracy:0.8287999939918518\n",
      "loss decreasing...\n",
      "Epoch: 2/4... Step: 500... Loss: 0.332231... Val Loss: 0.397897... Val Accuracy:0.822399994134903\n",
      "Epoch: 2/4... Step: 600... Loss: 0.328721... Val Loss: 0.397426... Val Accuracy:0.8131999945640564\n",
      "Epoch: 2/4... Step: 700... Loss: 0.459571... Val Loss: 0.389578... Val Accuracy:0.8343999934196472\n",
      "loss decreasing...\n",
      "Epoch: 2/4... Step: 800... Loss: 0.483896... Val Loss: 0.396164... Val Accuracy:0.8244000005722046\n",
      "Epoch: 3/4... Step: 900... Loss: 0.114501... Val Loss: 0.405948... Val Accuracy:0.8399999976158142\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.147044... Val Loss: 0.503659... Val Accuracy:0.8083999907970428\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.598083... Val Loss: 0.717111... Val Accuracy:0.7663999950885773\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.157518... Val Loss: 0.456084... Val Accuracy:0.8287999939918518\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.110399... Val Loss: 0.547024... Val Accuracy:0.823999992609024\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.183249... Val Loss: 0.528018... Val Accuracy:0.8211999928951264\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.145865... Val Loss: 0.578214... Val Accuracy:0.7855999994277955\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.179454... Val Loss: 0.547864... Val Accuracy:0.827599993944168\n",
      "min loss 0.389577568769\n",
      "Training time is: 203.89146900177002 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 1\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "net_train = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "if(train_on_gpu):\n",
    "    net_train.cuda()\n",
    "print(net_train)\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "from time import time\n",
    "start = time()\n",
    "train(net_train,epochs,train_loader,valid_loader,clip,print_every,0.01)\n",
    "print('Training time is:',time()-start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of positive:83.3600(1042/1250)\n",
      "Test Accuracy of negative:82.7200(1034/1250)\n",
      "Test Accuracy(Overall):83.0400 (2076/2500)\n",
      "Test loss: 0.408\n",
      "Test accuracy: 0.830\n",
      "It is a negative review\n",
      "It is a positive review\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 1\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "net.load_state_dict(torch.load('net_emotion_loss_min_old.pth'))\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "test(net,test_loader)\n",
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "predict(net, test_review_neg, sequence_length=100)\n",
    "\n",
    "\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "predict(net, test_review_pos, sequence_length=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
