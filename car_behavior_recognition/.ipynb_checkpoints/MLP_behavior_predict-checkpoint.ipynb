{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from time import time\n",
    "def labeltoint(label):\n",
    "    if label == 'left':\n",
    "        label = 0\n",
    "    if label == 'keep':\n",
    "        label = 1\n",
    "    if label == 'right':\n",
    "        label = 2\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['states', 'labels'])\n",
      "[0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 2, 0, 2, 0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 1, 2, 0, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 2, 1, 2, 0, 1, 1, 2, 2, 1, 2, 0, 1, 2, 1, 1, 0, 0, 2, 0, 1, 1, 0, 2, 2, 2, 0, 2, 0, 1, 1, 1, 0, 2, 2, 0, 1, 1, 2, 1, 1, 0, 2, 2, 1, 2, 2, 2, 2, 1, 0, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 0, 0, 0, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 1, 2, 0, 0, 2, 2, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 1, 0, 0, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 2, 2, 1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 1, 2, 1, 1, 0, 2, 0, 2, 0, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 2, 1, 0, 2, 2, 2, 1, 1, 2, 1, 0, 0, 2, 1, 1, 0, 0, 0, 2, 2, 0, 0, 2, 1, 0, 2, 0, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 0, 0, 0, 1, 1, 0, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 1, 0, 0, 0, 0, 2, 1, 2, 2, 2, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 0, 1, 0, 2, 1, 0, 0, 0, 2, 1, 2, 0, 2, 2, 0, 2, 2, 1, 2, 2, 2, 0, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 0, 2, 0, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 2, 2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 0, 2, 0, 2, 1, 0, 2, 1, 2, 1, 1, 0, 2, 2, 2, 1, 1, 2, 0, 2, 2, 1, 1, 2, 0, 0, 0, 1, 2, 1, 0, 2, 0, 2, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 0, 0, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 1, 0, 2, 0, 0, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 0, 1, 1, 1, 0, 2, 2, 2, 1, 1, 0, 0, 0, 0, 1, 1, 0, 2, 1, 1, 1, 2, 0, 1, 1, 2, 0, 0, 1, 0, 0, 1, 2, 0, 0, 2, 0, 0, 1, 0, 2, 1, 2, 2, 2, 1, 2, 2, 1, 0, 1, 2, 0, 2, 1, 0, 0, 0, 1, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2, 2, 2, 0, 2, 2, 0, 0, 2, 1, 1, 2, 0, 1, 2, 2, 0, 2, 2, 1, 1, 1, 1, 0, 0, 1, 0, 2, 1, 0, 1, 1, 2, 2, 0, 1, 2, 0, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 2, 0, 1, 1, 1, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 2, 1, 1, 1, 0, 0, 0, 2, 2, 1, 1, 0, 1, 0, 1, 1, 1, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open('data1/train.json', 'r') as f:\n",
    "    j = json.load(f)\n",
    "    print( j.keys())\n",
    "    X_train = j['states']\n",
    "    Y_train = j['labels']\n",
    "    for i in range(len(Y_train)):\n",
    "        Y_train[i] = labeltoint(Y_train[i])\n",
    "    print(Y_train)\n",
    "    \n",
    "with open('data1/test.json', 'r') as f:\n",
    "    j = json.load(f)\n",
    "    X_test = j['states']\n",
    "    Y_test = j['labels']\n",
    "    for i in range(len(Y_test)):\n",
    "        Y_test[i] = labeltoint(Y_test[i])\n",
    "    \n",
    "split_frac = 0.8\n",
    "X_train,Y_train,X_test,Y_test = np.array(X_train).astype(np.float32),np.array(Y_train).astype(np.long),np.array(X_test).astype(np.float32),np.array(Y_test).astype(np.long)\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "val_x,test_x = X_test[:len(X_test)//2],X_test[len(X_test)//2:]\n",
    "val_y,test_y = Y_test[:len(Y_test)//2],Y_test[len(Y_test)//2:]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    " \n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy((X_train)), torch.from_numpy(Y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from udacityplots import *\n",
    "import warnings\n",
    "import cv2\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import matplotlib \n",
    "matplotlib.use('agg')\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import SVC\n",
    "from  sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.ioff()\n",
    "\n",
    "def prettyPicture(clf, X_test, y_test):\n",
    "    x_min = 0.0; x_max = 1\n",
    "    y_min = 0.0; y_max = 1\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    h = 0.001  # step size in the mesh\n",
    "    \"\"\"\n",
    "    t1 = np.linspace(0, 1, 500)\n",
    "    t2 = np.linspace(0, 1, 500)\n",
    "    xx, yy = np.meshgrid(t1,t2)\n",
    "    \"\"\"\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    x_use = [[random.random() for i in range(0,2)] for  j in range(0,300)]             #xx.shape[0]*xx.shape[1])\n",
    "   # print(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z1 = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z0 = clf.predict(x_use)\n",
    "    Z = Z1.reshape(xx.shape)\n",
    "    #x_use = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \"\"\"\"\n",
    "    print(xx,yy,Z1,len(xx),len(yy),len(Z1),type(xx),end = '\\n')\n",
    "    print(xx.ravel(),yy.ravel(),len(xx.ravel()),len(yy.ravel()),type(xx.ravel()),end = '\\n')\n",
    "    print(np.c_[xx.ravel(), yy.ravel()],len(np.c_[xx.ravel(), yy.ravel()]),type(np.c_[xx.ravel(), yy.ravel()]),np.c_[xx.ravel(), yy.ravel()][0])\n",
    "    # Put the result into a color plot\n",
    "    print(xx.shape)\n",
    "    \n",
    "    #np.set_printoptions(threshold= 10000)\n",
    "    print(Z,len(Z[0]),len(Z))\n",
    "    \"\"\"\"\"\n",
    "    plt.xlim(xx.min(), xx.max())#图像所显示x轴的长度\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "     # use the data done by myself to predict the result and plot\n",
    "    plt.pcolormesh(xx, yy, Z, cmap= pl.cm.seismic)  #现在需要画出分类边界\n",
    "\n",
    "    grade_sig = [x_use[ii][0] for ii in range(0, len(x_use)) if Z0[ii] == 0]\n",
    "    bumpy_sig = [x_use[ii][1] for ii in range(0, len(x_use)) if Z0[ii] == 0]\n",
    "    grade_bkg = [x_use[ii][0] for ii in range(0, len(x_use)) if Z0[ii] == 1]\n",
    "    bumpy_bkg = [x_use[ii][1] for ii in range(0, len(x_use)) if Z0[ii] == 1]\n",
    "    grade1_bkg = [x_use[ii][0] for ii in range(0, len(x_use)) if Z0[ii] == 2]\n",
    "    bumpy1_bkg = [x_use[ii][1] for ii in range(0, len(x_use)) if Z0[ii] == 2]\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    # Plot also the test points\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "  \"\"\"\n",
    "    plt.scatter(grade_sig, bumpy_sig, color = \"b\", label=\"left\")\n",
    "    plt.scatter(grade_bkg, bumpy_bkg, color = \"r\", label=\"keep\")\n",
    "    plt.scatter(grade1_bkg, bumpy1_bkg, color = \"g\", label=\"right\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(\"bumpiness\")\n",
    "    plt.ylabel(\"grade\")\n",
    "    plt.show()\n",
    "    #plt.savefig(\"分类结果/test_k_means.png\")\n",
    "    #if  os.path.exists('分类结果/test_NB.png'):\n",
    "    #im1 = cv2.imread('分类结果/test_NB.png', 1)\n",
    "    #cv2.imshow('分类结果', im1)\n",
    "\n",
    "    #clf1 = SVC(kernel= kernel1())\n",
    "    #clf1.fit(X_test, y_test)\n",
    "    #if (clf == clf1):\n",
    "    plt.savefig(\"分类结果/test_svm_%s.png\" % kernel)\n",
    "    #if (clf == GaussianNB()):\n",
    "    #plt.savefig('分类结果/test_NB.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 213.31539 s\n",
      "the accuracy is : 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaohaipeng/.local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "#clf = SVC(C =0.8,gamma= 10000,kernel= kernel1())\n",
    "t0 = time()\n",
    "#### now your job is to fit the classifier\n",
    "#### using the training features/labels, and to\n",
    "#### make a set of predictions on the test data\n",
    "clf = SVC(C =10,kernel= 'poly',gamma=1)\n",
    "#features_train = features_train[:len(features_train)//10]\n",
    "#labels_train = labels_train[:len(labels_train)//10]\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"training time :\",round(time()-t0,5),'s')\n",
    "from sklearn.externals import joblib\n",
    "#save model\n",
    "joblib.dump(clf, 'svm_rbf.pkl')\n",
    "#### store your predictions in a list named pred\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, Y_test)\n",
    "print('the accuracy is :',acc)\n",
    "\n",
    "#output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 0.14316 s\n",
      "the accuracy is : 0.888\n"
     ]
    }
   ],
   "source": [
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "#clf = SVC(C =0.8,gamma= 10000,kernel= kernel1())\n",
    "t0 = time()\n",
    "#### now your job is to fit the classifier\n",
    "#### using the training features/labels, and to\n",
    "#### make a set of predictions on the test data\n",
    "clf = SVC(C =10,kernel= 'linear',gamma=1)\n",
    "#features_train = features_train[:len(features_train)//10]\n",
    "#labels_train = labels_train[:len(labels_train)//10]\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"training time :\",round(time()-t0,5),'s')\n",
    "from sklearn.externals import joblib\n",
    "#save model\n",
    "joblib.dump(clf, 'svm_rbf.pkl')\n",
    "#### store your predictions in a list named pred\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, Y_test)\n",
    "print('the accuracy is :',acc)\n",
    "\n",
    "#output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 0.05021 s\n",
      "the accuracy is : 0.772\n"
     ]
    }
   ],
   "source": [
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "#clf = SVC(C =0.8,gamma= 10000,kernel= kernel1())\n",
    "t0 = time()\n",
    "#### now your job is to fit the classifier\n",
    "#### using the training features/labels, and to\n",
    "#### make a set of predictions on the test data\n",
    "clf = SVC(C =10,kernel= 'rbf',gamma=1)\n",
    "#features_train = features_train[:len(features_train)//10]\n",
    "#labels_train = labels_train[:len(labels_train)//10]\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"training time :\",round(time()-t0,5),'s')\n",
    "from sklearn.externals import joblib\n",
    "#save model\n",
    "joblib.dump(clf, 'svm_rbf.pkl')\n",
    "#### store your predictions in a list named pred\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, Y_test)\n",
    "print('the accuracy is :',acc)\n",
    "\n",
    "#output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 0.0091 s\n",
      "the accuracy is : 0.396\n"
     ]
    }
   ],
   "source": [
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "#clf = SVC(C =0.8,gamma= 10000,kernel= kernel1())\n",
    "t0 = time()\n",
    "#### now your job is to fit the classifier\n",
    "#### using the training features/labels, and to\n",
    "#### make a set of predictions on the test data\n",
    "clf = SVC(C =10,kernel= 'sigmoid',gamma=1)\n",
    "#features_train = features_train[:len(features_train)//10]\n",
    "#labels_train = labels_train[:len(labels_train)//10]\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"training time :\",round(time()-t0,5),'s')\n",
    "from sklearn.externals import joblib\n",
    "#save model\n",
    "joblib.dump(clf, 'svm_rbf.pkl')\n",
    "#### store your predictions in a list named pred\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, Y_test)\n",
    "print('the accuracy is :',acc)\n",
    "\n",
    "#output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928\n"
     ]
    }
   ],
   "source": [
    "def classify(features_train, labels_train,features_test, labels_test):   \n",
    "    ### import the sklearn module for GaussianNB\n",
    "    ### create classifier\n",
    "    ### fit the classifier on the training features and labels\n",
    "    ### return the fit classifier\n",
    "    \n",
    "    \n",
    "    ### your code goes here!\n",
    "    from sklearn import tree\n",
    "    clf = tree.DecisionTreeClassifier(min_samples_split=8,criterion='entropy')\n",
    "    clf.fit(features_train, labels_train)\n",
    "    accuracy = clf.score(features_test, labels_test)\n",
    "    print(accuracy)\n",
    "   # prettyPicture(clf,features_test, labels_test)\n",
    "classify(X_train,Y_train,X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844\n"
     ]
    }
   ],
   "source": [
    "def classify(features_train, labels_train,features_test, labels_test):   \n",
    "    ### import the sklearn module for GaussianNB\n",
    "    ### create classifier\n",
    "    ### fit the classifier on the training features and labels\n",
    "    ### return the fit classifier\n",
    "    \n",
    "    \n",
    "    ### your code goes here!\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(features_train, labels_train)\n",
    "    accuracy = clf.score(features_test, labels_test)\n",
    "    print(accuracy)\n",
    "classify(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 0.02767 s\n",
      "the accuracy is : 0.356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "t0 = time()\n",
    "clf= KMeans(n_clusters=2, max_iter=1000,n_init=10,random_state=0).fit(X_train)\n",
    "#features_train = features_train[:len(features_train)//10]\n",
    "#labels_train = labels_train[:len(labels_train)//10]\n",
    "print(\"training time :\",round(time()-t0,5),'s')\n",
    "#### store your predictions in a list named pred\n",
    "pred = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, Y_test)\n",
    "print('the accuracy is :',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "#load model\n",
    "clf = joblib.load('svm_rbf.pkl')\n",
    "print(clf.predict([[0.2,0.6,0.2,-0.9],[0.6,0.63,0.9,0.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "def linear_batch_norm(input_size, output_size, batch_norm=False):\n",
    "    layer = []\n",
    "    linear = nn.Linear(input_size, output_size)\n",
    "    layer.append(linear)\n",
    "    if batch_norm:\n",
    "        batch_norm_ = nn.BatchNorm1d(output_size)\n",
    "        layer.append(batch_norm_)\n",
    "\n",
    "    return nn.Sequential(*layer)\n",
    "class trajectory_predict(nn.Module):\n",
    "    def __init__(self,input_size=16,conv_dim=32,out_size=2):\n",
    "        super(trajectory_predict,self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        self.conv_dim = input_size\n",
    "        self.fc1 = linear_batch_norm(input_size,conv_dim*4)\n",
    "        \n",
    "        self.fc2 = linear_batch_norm(conv_dim*4,conv_dim*8)\n",
    "        \n",
    "        self.fc3 = linear_batch_norm(conv_dim*8,conv_dim*4)\n",
    "        \"\"\"\n",
    "        self.fc4 = linear_batch_norm(conv_dim*4,conv_dim*4)\n",
    "        \n",
    "        self.fc5 = linear_batch_norm(conv_dim*4,conv_dim*2)\n",
    "        self.fc6 = linear_batch_norm(conv_dim*2,conv_dim*2)\n",
    "        self.fc7 = linear_batch_norm(conv_dim*2,conv_dim)\n",
    "        self.fc8 = linear_batch_norm(conv_dim,conv_dim*4)\n",
    "       \n",
    "        self.fc9 = linear_batch_norm(conv_dim*4,conv_dim*8)\n",
    "        self.fc10 = linear_batch_norm(conv_dim*8,conv_dim*4)\n",
    "        \n",
    "        self.fc11 = linear_batch_norm(conv_dim*4,conv_dim*4)\n",
    "        self.fc12 = linear_batch_norm(conv_dim*4,conv_dim*3)\n",
    "        self.fc13 = linear_batch_norm(conv_dim*3,conv_dim*2)\n",
    "        self.fc14 = linear_batch_norm(conv_dim*2,conv_dim*4)\n",
    "        self.fc15 = linear_batch_norm(conv_dim*4,conv_dim*2)\n",
    "        self.fc16 = linear_batch_norm(conv_dim*2,conv_dim*2)\n",
    "        self.fc17 = linear_batch_norm(conv_dim*2,conv_dim*4)\n",
    "        self.fc18 = linear_batch_norm(conv_dim*4,conv_dim*2)\n",
    "        self.fc19 = linear_batch_norm(conv_dim*2,conv_dim*2)\n",
    "        self.fc20 = linear_batch_norm(conv_dim*2,conv_dim*4)\n",
    "        \"\"\"\n",
    "        self.fc21 = linear_batch_norm(conv_dim*4,out_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.leakrelu = nn.LeakyReLU(0)\n",
    "        self.drop = nn.Dropout(0.02)\n",
    "    def forward(self,x):\n",
    "        x = self.leakrelu(self.fc1(x))\n",
    "        x = self.drop(x) \n",
    "        x = self.leakrelu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc3(x))\n",
    "        x = self.drop(x)\n",
    "        \"\"\"\n",
    "        x = self.leakrelu(self.fc4(x))\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.leakrelu(self.fc5(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc6(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc7(x))\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.leakrelu(self.fc8(x))\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.leakrelu(self.fc9(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc10(x))\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.leakrelu(self.fc11(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc12(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc13(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc14(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc15(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc16(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc17(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.leakrelu(self.fc18(x))\n",
    "        x = self.drop(x) \n",
    "        x = self.leakrelu(self.fc19(x))\n",
    "        x = self.drop(x) \n",
    "        x = self.leakrelu(self.fc20(x))\n",
    "        x = self.drop(x)\n",
    "        \"\"\"\n",
    "        x = self.fc21(x)\n",
    "        #x = self.softmax(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x\n",
    "net_predict = trajectory_predict(X_train.shape[1],256,3)\n",
    "#net_predict.load_state_dict(torch.load('net_loss_min.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net_predict.parameters(),lr=0.00128)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=100,gamma=0.8)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.5,\n",
    "patience=50,verbose=True,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    net_predict.cuda()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def train(net,train_loader,valid_loader,epoches=10,print_every=2000):\n",
    "    loss_min = np.inf\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    accuracies_e = []\n",
    "    for e in range(1,epoches+1):\n",
    "        loss_train = []\n",
    "        loss_valid = []\n",
    "        accuracies = []\n",
    "        for batch_i,(x,y) in enumerate(train_loader):\n",
    "            if use_cuda:\n",
    "                x, y = x.cuda(),y.cuda()\n",
    "            out = net(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(out,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train.append(loss.item())\n",
    "            \n",
    "         #   if batch_i % print_every == 0:\n",
    "        net.eval()\n",
    "        for x,y in valid_loader:\n",
    "            if use_cuda:\n",
    "                x,y = x.cuda(),y.cuda()\n",
    "            out = net(x)\n",
    "            _,class_ = torch.max(out,dim=1)\n",
    "            equal = class_ == y.view(class_.shape)\n",
    "            accuracy = torch.mean(equal.type(torch.FloatTensor)).item()\n",
    "            loss = criterion(out,y)\n",
    "            loss_valid.append(loss.item())\n",
    "            accuracies.append(accuracy)\n",
    "        losses_train.append(np.mean(loss_train))\n",
    "        losses_valid.append(np.mean(loss_valid))\n",
    "        accuracies_e.append(np.mean(accuracies))\n",
    "        print('{}/{}'.format(e,epoches),'Train Loss:',np.mean(loss_train),'Valid Loss:',np.mean(loss_valid),'Valid Accuracy:',np.mean(accuracies))\n",
    "        if loss_min > np.mean(loss_valid):     \n",
    "            print('Loss decrease...')\n",
    "            loss_min = np.mean(loss_valid)\n",
    "            torch.save(net.state_dict(),'model/net_bahavior_loss_min_mlp.pth')\n",
    "            \n",
    "        \n",
    "        net.train()\n",
    "        #scheduler.step(np.mean(loss_valid))\n",
    "        torch.save(net.state_dict(),'model/net_behavior_mlp.pth')\n",
    "    print('min loss:',loss_min)\n",
    "   # plt.plot(losses_train,color='r',label='train_loss')\n",
    "    #plt.plot(losses_valid,color='g',label='valid_loss')\n",
    "    plt.plot(losses_train,color='r',label='训练损失')\n",
    "    plt.plot(losses_valid,color='g',label='验证损失')\n",
    "    #plt.title('Loss_Trend')\n",
    "    #plt.xlabel('Epoches')\n",
    "    #plt.ylabel('Loss')\n",
    "    plt.title('损失变化')\n",
    "    plt.xlabel('迭代次数')\n",
    "    plt.ylabel('损失大小')\n",
    "    plt.legend()\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.savefig('behavior_image/loss1.svg',dpi=300)\n",
    "    plt.savefig('behavior_image/loss1.png',dpi=300)\n",
    "    return accuracies_e\n",
    "      \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/350 Train Loss: 3.5321834683418274 Valid Loss: 1.1012709736824036 Valid Accuracy: 0.3365778625011444\n",
      "Loss decrease...\n",
      "2/350 Train Loss: 1.061871627966563 Valid Loss: 1.0211881399154663 Valid Accuracy: 0.47207991778850555\n",
      "Loss decrease...\n",
      "3/350 Train Loss: 0.9543761064608892 Valid Loss: 0.9132672548294067 Valid Accuracy: 0.4802766442298889\n",
      "Loss decrease...\n",
      "4/350 Train Loss: 0.8101602296034495 Valid Loss: 0.6768592894077301 Valid Accuracy: 0.6950563490390778\n",
      "Loss decrease...\n",
      "5/350 Train Loss: 0.6431538710991541 Valid Loss: 0.9795093834400177 Valid Accuracy: 0.5193391442298889\n",
      "6/350 Train Loss: 0.7873058021068573 Valid Loss: 0.511676162481308 Valid Accuracy: 0.8317110538482666\n",
      "Loss decrease...\n",
      "7/350 Train Loss: 0.5373599504431089 Valid Loss: 0.38836997747421265 Valid Accuracy: 0.8410604596138\n",
      "Loss decrease...\n",
      "8/350 Train Loss: 0.4386746933062871 Valid Loss: 0.3321074768900871 Valid Accuracy: 0.8785860538482666\n",
      "Loss decrease...\n",
      "9/350 Train Loss: 0.3446320767203967 Valid Loss: 0.29788510501384735 Valid Accuracy: 0.8801229596138\n",
      "Loss decrease...\n",
      "10/350 Train Loss: 0.28262966747085255 Valid Loss: 0.3337220251560211 Valid Accuracy: 0.8399077951908112\n",
      "11/350 Train Loss: 0.33420120055476826 Valid Loss: 0.25469450652599335 Valid Accuracy: 0.8723104596138\n",
      "Loss decrease...\n",
      "12/350 Train Loss: 0.25342878326773643 Valid Loss: 0.25471244752407074 Valid Accuracy: 0.8875512182712555\n",
      "13/350 Train Loss: 0.2641036535302798 Valid Loss: 0.27076858282089233 Valid Accuracy: 0.8559170067310333\n",
      "14/350 Train Loss: 0.27465929339329404 Valid Loss: 0.5864022672176361 Valid Accuracy: 0.7777920067310333\n",
      "15/350 Train Loss: 0.2785600697000821 Valid Loss: 0.305215522646904 Valid Accuracy: 0.8160860538482666\n",
      "16/350 Train Loss: 0.25238023201624554 Valid Loss: 0.19654934108257294 Valid Accuracy: 0.9043288826942444\n",
      "Loss decrease...\n",
      "17/350 Train Loss: 0.23491671308875084 Valid Loss: 0.21373175829648972 Valid Accuracy: 0.8723104596138\n",
      "18/350 Train Loss: 0.2432598409553369 Valid Loss: 0.3339691311120987 Valid Accuracy: 0.8336321711540222\n",
      "19/350 Train Loss: 0.19322733084360758 Valid Loss: 0.18091483414173126 Valid Accuracy: 0.9281506240367889\n",
      "Loss decrease...\n",
      "20/350 Train Loss: 0.26033736517031986 Valid Loss: 0.24881064146757126 Valid Accuracy: 0.8551485538482666\n",
      "21/350 Train Loss: 0.23845363408327103 Valid Loss: 0.25942572951316833 Valid Accuracy: 0.8641137182712555\n",
      "22/350 Train Loss: 0.27969830359021824 Valid Loss: 0.21687150746583939 Valid Accuracy: 0.9050973355770111\n",
      "23/350 Train Loss: 0.20858012388149896 Valid Loss: 0.17121753096580505 Valid Accuracy: 0.9351946711540222\n",
      "Loss decrease...\n",
      "24/350 Train Loss: 0.19741356497009596 Valid Loss: 0.23610582947731018 Valid Accuracy: 0.8887038826942444\n",
      "25/350 Train Loss: 0.2129356854905685 Valid Loss: 0.22268615663051605 Valid Accuracy: 0.8808913826942444\n",
      "26/350 Train Loss: 0.1941732782870531 Valid Loss: 0.2529042214155197 Valid Accuracy: 0.8648821711540222\n",
      "27/350 Train Loss: 0.22894476105769476 Valid Loss: 0.1847231090068817 Valid Accuracy: 0.9047131240367889\n",
      "28/350 Train Loss: 0.18829927717645964 Valid Loss: 0.2182895466685295 Valid Accuracy: 0.8871670067310333\n",
      "29/350 Train Loss: 0.2103471253067255 Valid Loss: 0.2512657195329666 Valid Accuracy: 0.8789702951908112\n",
      "30/350 Train Loss: 0.19505930816133818 Valid Loss: 0.2462327927350998 Valid Accuracy: 0.8726946711540222\n",
      "31/350 Train Loss: 0.24963630239168802 Valid Loss: 0.3014361262321472 Valid Accuracy: 0.8332479596138\n",
      "32/350 Train Loss: 0.22727071369687715 Valid Loss: 0.17336230725049973 Valid Accuracy: 0.9035604596138\n",
      "33/350 Train Loss: 0.19845012792696556 Valid Loss: 0.16634002327919006 Valid Accuracy: 0.9121413826942444\n",
      "Loss decrease...\n",
      "34/350 Train Loss: 0.18537278970082602 Valid Loss: 0.13792143389582634 Valid Accuracy: 0.9437756240367889\n",
      "Loss decrease...\n",
      "35/350 Train Loss: 0.17350883223116398 Valid Loss: 0.26219210773706436 Valid Accuracy: 0.8738473355770111\n",
      "36/350 Train Loss: 0.2005031257867813 Valid Loss: 0.16120585054159164 Valid Accuracy: 0.9281506240367889\n",
      "37/350 Train Loss: 0.1782259320219358 Valid Loss: 0.1992175504565239 Valid Accuracy: 0.8883196711540222\n",
      "38/350 Train Loss: 0.1584965114792188 Valid Loss: 0.13391636312007904 Valid Accuracy: 0.9441598355770111\n",
      "Loss decrease...\n",
      "39/350 Train Loss: 0.17277224423984686 Valid Loss: 0.1471455693244934 Valid Accuracy: 0.9355788826942444\n",
      "40/350 Train Loss: 0.15139802607397237 Valid Loss: 0.13444199785590172 Valid Accuracy: 0.9515881240367889\n",
      "41/350 Train Loss: 0.2013041004538536 Valid Loss: 0.21717356145381927 Valid Accuracy: 0.8797387182712555\n",
      "42/350 Train Loss: 0.19302925591667494 Valid Loss: 0.1446188986301422 Valid Accuracy: 0.9441598355770111\n",
      "43/350 Train Loss: 0.1580144582937161 Valid Loss: 0.15912961214780807 Valid Accuracy: 0.9277663826942444\n",
      "44/350 Train Loss: 0.16838507975141206 Valid Loss: 0.12797780707478523 Valid Accuracy: 0.9597848355770111\n",
      "Loss decrease...\n",
      "45/350 Train Loss: 0.16958903521299362 Valid Loss: 0.2513403296470642 Valid Accuracy: 0.8723104596138\n",
      "46/350 Train Loss: 0.2096412262568871 Valid Loss: 0.15386251360177994 Valid Accuracy: 0.9281506240367889\n",
      "47/350 Train Loss: 0.20299135583142439 Valid Loss: 0.15125034004449844 Valid Accuracy: 0.9519723355770111\n",
      "48/350 Train Loss: 0.17488406846920648 Valid Loss: 0.17016392946243286 Valid Accuracy: 0.9371157884597778\n",
      "49/350 Train Loss: 0.13758569893737635 Valid Loss: 0.16733618080615997 Valid Accuracy: 0.9363473355770111\n",
      "50/350 Train Loss: 0.15662906815608343 Valid Loss: 0.11701388284564018 Valid Accuracy: 0.9519723355770111\n",
      "Loss decrease...\n",
      "51/350 Train Loss: 0.1718787346035242 Valid Loss: 0.2815587669610977 Valid Accuracy: 0.8637295067310333\n",
      "52/350 Train Loss: 0.14721298217773438 Valid Loss: 0.13696398586034775 Valid Accuracy: 0.9359631240367889\n",
      "53/350 Train Loss: 0.13122810671726862 Valid Loss: 0.12635483592748642 Valid Accuracy: 0.9597848355770111\n",
      "54/350 Train Loss: 0.1469962348540624 Valid Loss: 0.15784046798944473 Valid Accuracy: 0.9117571711540222\n",
      "55/350 Train Loss: 0.13861337738732496 Valid Loss: 0.15516777709126472 Valid Accuracy: 0.9031762182712555\n",
      "56/350 Train Loss: 0.15540065802633762 Valid Loss: 0.14326872304081917 Valid Accuracy: 0.9195696711540222\n",
      "57/350 Train Loss: 0.13141736450294653 Valid Loss: 0.10698531195521355 Valid Accuracy: 0.9597848355770111\n",
      "Loss decrease...\n",
      "58/350 Train Loss: 0.14161461405456066 Valid Loss: 0.1191694438457489 Valid Accuracy: 0.9281506240367889\n",
      "59/350 Train Loss: 0.16578460236390433 Valid Loss: 0.2114594653248787 Valid Accuracy: 0.8957479596138\n",
      "60/350 Train Loss: 0.13868770996729532 Valid Loss: 0.1472897045314312 Valid Accuracy: 0.9207223355770111\n",
      "61/350 Train Loss: 0.13698812325795492 Valid Loss: 0.13172893226146698 Valid Accuracy: 0.9199538826942444\n",
      "62/350 Train Loss: 0.14582978126903376 Valid Loss: 0.10868526250123978 Valid Accuracy: 0.9515881240367889\n",
      "63/350 Train Loss: 0.12661567889153957 Valid Loss: 0.10400966927409172 Valid Accuracy: 0.9523565471172333\n",
      "Loss decrease...\n",
      "64/350 Train Loss: 0.1497949305921793 Valid Loss: 0.17348017543554306 Valid Accuracy: 0.9203381240367889\n",
      "65/350 Train Loss: 0.16801384029289088 Valid Loss: 0.20159757882356644 Valid Accuracy: 0.8965163826942444\n",
      "66/350 Train Loss: 0.15379137359559536 Valid Loss: 0.19045550376176834 Valid Accuracy: 0.9043288826942444\n",
      "67/350 Train Loss: 0.1624296779433886 Valid Loss: 0.1255100816488266 Valid Accuracy: 0.9363473355770111\n",
      "68/350 Train Loss: 0.14704986910025278 Valid Loss: 0.16750472411513329 Valid Accuracy: 0.9109887182712555\n",
      "69/350 Train Loss: 0.14392094686627388 Valid Loss: 0.11891024559736252 Valid Accuracy: 0.9433913826942444\n",
      "70/350 Train Loss: 0.12745911814272404 Valid Loss: 0.13016485050320625 Valid Accuracy: 0.9277663826942444\n",
      "71/350 Train Loss: 0.1412954324235519 Valid Loss: 0.1456492803990841 Valid Accuracy: 0.9355788826942444\n",
      "72/350 Train Loss: 0.11786376653860013 Valid Loss: 0.14105744659900665 Valid Accuracy: 0.9285348355770111\n",
      "73/350 Train Loss: 0.1621380147213737 Valid Loss: 0.11395683884620667 Valid Accuracy: 0.9519723355770111\n",
      "74/350 Train Loss: 0.11233115755021572 Valid Loss: 0.10772263631224632 Valid Accuracy: 0.9433913826942444\n",
      "75/350 Train Loss: 0.11585074849426746 Valid Loss: 0.1148534044623375 Valid Accuracy: 0.9523565471172333\n",
      "76/350 Train Loss: 0.11903456598520279 Valid Loss: 0.11080395057797432 Valid Accuracy: 0.9441598355770111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/350 Train Loss: 0.12808918642501035 Valid Loss: 0.12134324759244919 Valid Accuracy: 0.9441598355770111\n",
      "78/350 Train Loss: 0.10952836740761995 Valid Loss: 0.10263604298233986 Valid Accuracy: 0.9355788826942444\n",
      "Loss decrease...\n",
      "79/350 Train Loss: 0.10143404950698216 Valid Loss: 0.09457282349467278 Valid Accuracy: 0.9679815471172333\n",
      "Loss decrease...\n",
      "80/350 Train Loss: 0.10741117720802625 Valid Loss: 0.10520058125257492 Valid Accuracy: 0.9437756240367889\n",
      "81/350 Train Loss: 0.13144243570665518 Valid Loss: 0.10842946544289589 Valid Accuracy: 0.9683657884597778\n",
      "82/350 Train Loss: 0.13283467075477043 Valid Loss: 0.14662109315395355 Valid Accuracy: 0.9363473355770111\n",
      "83/350 Train Loss: 0.14993130912383398 Valid Loss: 0.1282455176115036 Valid Accuracy: 0.9277663826942444\n",
      "84/350 Train Loss: 0.11301068651179473 Valid Loss: 0.10844740271568298 Valid Accuracy: 0.9597848355770111\n",
      "85/350 Train Loss: 0.1407359087218841 Valid Loss: 0.13003910705447197 Valid Accuracy: 0.9437756240367889\n",
      "86/350 Train Loss: 0.12718885019421577 Valid Loss: 0.12838643789291382 Valid Accuracy: 0.9371157884597778\n",
      "87/350 Train Loss: 0.14039145347972712 Valid Loss: 0.12096743285655975 Valid Accuracy: 0.9437756240367889\n",
      "88/350 Train Loss: 0.1582016764829556 Valid Loss: 0.111797284334898 Valid Accuracy: 0.9519723355770111\n",
      "89/350 Train Loss: 0.14000146153072515 Valid Loss: 0.10999433696269989 Valid Accuracy: 0.9527407884597778\n",
      "90/350 Train Loss: 0.13055282644927502 Valid Loss: 0.1477917544543743 Valid Accuracy: 0.9113729596138\n",
      "91/350 Train Loss: 0.12232215392092864 Valid Loss: 0.16057509183883667 Valid Accuracy: 0.9195696711540222\n",
      "92/350 Train Loss: 0.10781656050433715 Valid Loss: 0.09481573849916458 Valid Accuracy: 0.9519723355770111\n",
      "93/350 Train Loss: 0.11637097379813592 Valid Loss: 0.10512883961200714 Valid Accuracy: 0.9437756240367889\n",
      "94/350 Train Loss: 0.10695875808596611 Valid Loss: 0.09572042152285576 Valid Accuracy: 0.9519723355770111\n",
      "95/350 Train Loss: 0.09812694819023211 Valid Loss: 0.1166614219546318 Valid Accuracy: 0.9523565471172333\n",
      "96/350 Train Loss: 0.09592001900697748 Valid Loss: 0.14321215450763702 Valid Accuracy: 0.9355788826942444\n",
      "97/350 Train Loss: 0.12085285875946283 Valid Loss: 0.10503153130412102 Valid Accuracy: 0.9281506240367889\n",
      "98/350 Train Loss: 0.11689074492702882 Valid Loss: 0.10873088240623474 Valid Accuracy: 0.9449282884597778\n",
      "99/350 Train Loss: 0.10841500588382284 Valid Loss: 0.11744877696037292 Valid Accuracy: 0.9348104596138\n",
      "100/350 Train Loss: 0.13210989845295748 Valid Loss: 0.21772916615009308 Valid Accuracy: 0.8957479596138\n",
      "101/350 Train Loss: 0.22402341042955717 Valid Loss: 0.19028906524181366 Valid Accuracy: 0.8957479596138\n",
      "102/350 Train Loss: 0.16534179573257765 Valid Loss: 0.12034245952963829 Valid Accuracy: 0.9597848355770111\n",
      "103/350 Train Loss: 0.1419932165493568 Valid Loss: 0.15100739151239395 Valid Accuracy: 0.9281506240367889\n",
      "104/350 Train Loss: 0.11992448413123687 Valid Loss: 0.11857462301850319 Valid Accuracy: 0.9527407884597778\n",
      "105/350 Train Loss: 0.11162334121763706 Valid Loss: 0.1277323067188263 Valid Accuracy: 0.9527407884597778\n",
      "106/350 Train Loss: 0.10894436482340097 Valid Loss: 0.21629831194877625 Valid Accuracy: 0.9043288826942444\n",
      "107/350 Train Loss: 0.12821531606217226 Valid Loss: 0.10541583597660065 Valid Accuracy: 0.9675973355770111\n",
      "108/350 Train Loss: 0.11988104072709878 Valid Loss: 0.12234579399228096 Valid Accuracy: 0.9437756240367889\n",
      "109/350 Train Loss: 0.1545418631285429 Valid Loss: 0.11997146159410477 Valid Accuracy: 0.9605532884597778\n",
      "110/350 Train Loss: 0.15096787363290787 Valid Loss: 0.1937088444828987 Valid Accuracy: 0.8961321711540222\n",
      "111/350 Train Loss: 0.12757541860143343 Valid Loss: 0.14417333155870438 Valid Accuracy: 0.9277663826942444\n",
      "112/350 Train Loss: 0.11292932586123546 Valid Loss: 0.14000600948929787 Valid Accuracy: 0.9523565471172333\n",
      "113/350 Train Loss: 0.10838681894044082 Valid Loss: 0.1743006631731987 Valid Accuracy: 0.9199538826942444\n",
      "114/350 Train Loss: 0.10717048837492864 Valid Loss: 0.10559830814599991 Valid Accuracy: 0.9363473355770111\n",
      "115/350 Train Loss: 0.09900760200495522 Valid Loss: 0.09872573241591454 Valid Accuracy: 0.9519723355770111\n",
      "116/350 Train Loss: 0.1342022924994429 Valid Loss: 0.13479077816009521 Valid Accuracy: 0.9277663826942444\n",
      "117/350 Train Loss: 0.11236283493538697 Valid Loss: 0.13149166107177734 Valid Accuracy: 0.9527407884597778\n",
      "118/350 Train Loss: 0.10335287017126878 Valid Loss: 0.09403208829462528 Valid Accuracy: 0.9449282884597778\n",
      "Loss decrease...\n",
      "119/350 Train Loss: 0.09725868081053098 Valid Loss: 0.09103172272443771 Valid Accuracy: 0.9523565471172333\n",
      "Loss decrease...\n",
      "120/350 Train Loss: 0.09245962214966615 Valid Loss: 0.09932483360171318 Valid Accuracy: 0.9675973355770111\n",
      "121/350 Train Loss: 0.10242567087213199 Valid Loss: 0.11126219481229782 Valid Accuracy: 0.9277663826942444\n",
      "122/350 Train Loss: 0.09724018971125285 Valid Loss: 0.1229066289961338 Valid Accuracy: 0.9519723355770111\n",
      "123/350 Train Loss: 0.08962038749208052 Valid Loss: 0.13118183985352516 Valid Accuracy: 0.9348104596138\n",
      "124/350 Train Loss: 0.0975149953737855 Valid Loss: 0.09929228946566582 Valid Accuracy: 0.9437756240367889\n",
      "125/350 Train Loss: 0.1516068222311636 Valid Loss: 0.12448940053582191 Valid Accuracy: 0.9344262182712555\n",
      "126/350 Train Loss: 0.1403751273949941 Valid Loss: 0.11757538467645645 Valid Accuracy: 0.9523565471172333\n",
      "127/350 Train Loss: 0.12529212484757105 Valid Loss: 0.10988970473408699 Valid Accuracy: 0.9437756240367889\n",
      "128/350 Train Loss: 0.11656896521647771 Valid Loss: 0.10491669550538063 Valid Accuracy: 0.9597848355770111\n",
      "129/350 Train Loss: 0.09865189017727971 Valid Loss: 0.10852469503879547 Valid Accuracy: 0.9441598355770111\n",
      "130/350 Train Loss: 0.09945721365511417 Valid Loss: 0.11120576038956642 Valid Accuracy: 0.9527407884597778\n",
      "131/350 Train Loss: 0.12880474220340452 Valid Loss: 0.122120700776577 Valid Accuracy: 0.9527407884597778\n",
      "132/350 Train Loss: 0.12233364302664995 Valid Loss: 0.12566443905234337 Valid Accuracy: 0.9519723355770111\n",
      "133/350 Train Loss: 0.11180698033422232 Valid Loss: 0.09315795078873634 Valid Accuracy: 0.9679815471172333\n",
      "134/350 Train Loss: 0.12731950699041286 Valid Loss: 0.08906351216137409 Valid Accuracy: 0.9523565471172333\n",
      "Loss decrease...\n",
      "135/350 Train Loss: 0.11320564119766156 Valid Loss: 0.13894709944725037 Valid Accuracy: 0.9277663826942444\n",
      "136/350 Train Loss: 0.12672238827993473 Valid Loss: 0.1079527847468853 Valid Accuracy: 0.9527407884597778\n",
      "137/350 Train Loss: 0.09616448575009902 Valid Loss: 0.1199246384203434 Valid Accuracy: 0.9523565471172333\n",
      "138/350 Train Loss: 0.08759630626688401 Valid Loss: 0.10793207585811615 Valid Accuracy: 0.9523565471172333\n",
      "139/350 Train Loss: 0.08396582398563623 Valid Loss: 0.15626055374741554 Valid Accuracy: 0.9136782884597778\n",
      "140/350 Train Loss: 0.11599609659363826 Valid Loss: 0.22643814980983734 Valid Accuracy: 0.9024077951908112\n",
      "141/350 Train Loss: 0.11927149848391612 Valid Loss: 0.13128270953893661 Valid Accuracy: 0.9437756240367889\n",
      "142/350 Train Loss: 0.08943778680016597 Valid Loss: 0.11696605011820793 Valid Accuracy: 0.9281506240367889\n",
      "143/350 Train Loss: 0.09216299559921026 Valid Loss: 0.1090484969317913 Valid Accuracy: 0.9437756240367889\n",
      "144/350 Train Loss: 0.09275713888928294 Valid Loss: 0.12459037825465202 Valid Accuracy: 0.9281506240367889\n",
      "145/350 Train Loss: 0.10111193809037407 Valid Loss: 0.11967353522777557 Valid Accuracy: 0.9594006240367889\n",
      "146/350 Train Loss: 0.08358748878041904 Valid Loss: 0.11378537490963936 Valid Accuracy: 0.9445440471172333\n",
      "147/350 Train Loss: 0.08710640948265791 Valid Loss: 0.11122646927833557 Valid Accuracy: 0.9594006240367889\n",
      "148/350 Train Loss: 0.10299876642723878 Valid Loss: 0.12311598658561707 Valid Accuracy: 0.9515881240367889\n",
      "149/350 Train Loss: 0.11313784991701444 Valid Loss: 0.09193146787583828 Valid Accuracy: 0.9523565471172333\n",
      "150/350 Train Loss: 0.11004690950115521 Valid Loss: 0.13143078610301018 Valid Accuracy: 0.9285348355770111\n",
      "151/350 Train Loss: 0.11847998605420192 Valid Loss: 0.10488059744238853 Valid Accuracy: 0.9441598355770111\n",
      "152/350 Train Loss: 0.10784644975016515 Valid Loss: 0.10363881289958954 Valid Accuracy: 0.9266137182712555\n",
      "153/350 Train Loss: 0.09698220870147149 Valid Loss: 0.12481718510389328 Valid Accuracy: 0.9515881240367889\n",
      "154/350 Train Loss: 0.09635568503290415 Valid Loss: 0.11686382442712784 Valid Accuracy: 0.9359631240367889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/350 Train Loss: 0.10093018676464756 Valid Loss: 0.18049778789281845 Valid Accuracy: 0.9113729596138\n",
      "156/350 Train Loss: 0.12741863851745924 Valid Loss: 0.1310812160372734 Valid Accuracy: 0.9519723355770111\n",
      "157/350 Train Loss: 0.11782704200595617 Valid Loss: 0.15918351709842682 Valid Accuracy: 0.9199538826942444\n",
      "158/350 Train Loss: 0.09735388805468877 Valid Loss: 0.12198984995484352 Valid Accuracy: 0.9207223355770111\n",
      "159/350 Train Loss: 0.09970862232148647 Valid Loss: 0.14322136342525482 Valid Accuracy: 0.9437756240367889\n",
      "160/350 Train Loss: 0.09179006703197956 Valid Loss: 0.12062440440058708 Valid Accuracy: 0.9449282884597778\n",
      "161/350 Train Loss: 0.08060578582808375 Valid Loss: 0.11661339923739433 Valid Accuracy: 0.9359631240367889\n",
      "162/350 Train Loss: 0.08365978828320901 Valid Loss: 0.11246386542916298 Valid Accuracy: 0.9601690471172333\n",
      "163/350 Train Loss: 0.11192093417048454 Valid Loss: 0.1476442813873291 Valid Accuracy: 0.9355788826942444\n",
      "164/350 Train Loss: 0.10989553015679121 Valid Loss: 0.16651369631290436 Valid Accuracy: 0.9355788826942444\n",
      "165/350 Train Loss: 0.10729763905207317 Valid Loss: 0.13061808049678802 Valid Accuracy: 0.9590163826942444\n",
      "166/350 Train Loss: 0.0731093417853117 Valid Loss: 0.17985066026449203 Valid Accuracy: 0.9281506240367889\n",
      "167/350 Train Loss: 0.0816291612572968 Valid Loss: 0.1197637990117073 Valid Accuracy: 0.9515881240367889\n",
      "168/350 Train Loss: 0.07221068472911914 Valid Loss: 0.1097057182341814 Valid Accuracy: 0.9675973355770111\n",
      "169/350 Train Loss: 0.07345927599817514 Valid Loss: 0.12159956246614456 Valid Accuracy: 0.9445440471172333\n",
      "170/350 Train Loss: 0.09623642436539133 Valid Loss: 0.11304153874516487 Valid Accuracy: 0.9433913826942444\n",
      "171/350 Train Loss: 0.0780533142387867 Valid Loss: 0.11015987768769264 Valid Accuracy: 0.9597848355770111\n",
      "172/350 Train Loss: 0.0843866253271699 Valid Loss: 0.08679206669330597 Valid Accuracy: 0.9515881240367889\n",
      "Loss decrease...\n",
      "173/350 Train Loss: 0.0753042825187246 Valid Loss: 0.10116465389728546 Valid Accuracy: 0.9433913826942444\n",
      "174/350 Train Loss: 0.10932394303381443 Valid Loss: 0.13759614154696465 Valid Accuracy: 0.9441598355770111\n",
      "175/350 Train Loss: 0.11040738659600417 Valid Loss: 0.14104494079947472 Valid Accuracy: 0.9590163826942444\n",
      "176/350 Train Loss: 0.10350250359624624 Valid Loss: 0.15798136591911316 Valid Accuracy: 0.9281506240367889\n",
      "177/350 Train Loss: 0.09739044836411874 Valid Loss: 0.11039166525006294 Valid Accuracy: 0.9527407884597778\n",
      "178/350 Train Loss: 0.08494249545037746 Valid Loss: 0.14927330240607262 Valid Accuracy: 0.9285348355770111\n",
      "179/350 Train Loss: 0.08466177821780245 Valid Loss: 0.11764141544699669 Valid Accuracy: 0.9449282884597778\n",
      "180/350 Train Loss: 0.09234900275866191 Valid Loss: 0.13478884100914001 Valid Accuracy: 0.9437756240367889\n",
      "181/350 Train Loss: 0.07625020109117031 Valid Loss: 0.13282828405499458 Valid Accuracy: 0.9359631240367889\n",
      "182/350 Train Loss: 0.07352227407197158 Valid Loss: 0.13175080344080925 Valid Accuracy: 0.9433913826942444\n",
      "183/350 Train Loss: 0.09523557685315609 Valid Loss: 0.12848805263638496 Valid Accuracy: 0.9289190471172333\n",
      "184/350 Train Loss: 0.07749073176334302 Valid Loss: 0.1388167105615139 Valid Accuracy: 0.9437756240367889\n",
      "185/350 Train Loss: 0.07136598043143749 Valid Loss: 0.15921127796173096 Valid Accuracy: 0.9359631240367889\n",
      "186/350 Train Loss: 0.07793968512366216 Valid Loss: 0.11669702082872391 Valid Accuracy: 0.9449282884597778\n",
      "187/350 Train Loss: 0.0876664115736882 Valid Loss: 0.1325870230793953 Valid Accuracy: 0.9597848355770111\n",
      "188/350 Train Loss: 0.08989149316524465 Valid Loss: 0.10256354883313179 Valid Accuracy: 0.9601690471172333\n",
      "189/350 Train Loss: 0.0745595529054602 Valid Loss: 0.11918578669428825 Valid Accuracy: 0.9527407884597778\n",
      "190/350 Train Loss: 0.07779409689828753 Valid Loss: 0.1074722558259964 Valid Accuracy: 0.9527407884597778\n",
      "191/350 Train Loss: 0.07783831935375929 Valid Loss: 0.11604360491037369 Valid Accuracy: 0.9527407884597778\n",
      "192/350 Train Loss: 0.09115139612307151 Valid Loss: 0.1282050721347332 Valid Accuracy: 0.9359631240367889\n",
      "193/350 Train Loss: 0.0779838024949034 Valid Loss: 0.11965138744562864 Valid Accuracy: 0.9590163826942444\n",
      "194/350 Train Loss: 0.10667890403419733 Valid Loss: 0.19077126309275627 Valid Accuracy: 0.9293032884597778\n",
      "195/350 Train Loss: 0.0984661327674985 Valid Loss: 0.14180773124098778 Valid Accuracy: 0.9519723355770111\n",
      "196/350 Train Loss: 0.09463959001004696 Valid Loss: 0.13906678557395935 Valid Accuracy: 0.9675973355770111\n",
      "197/350 Train Loss: 0.09183545286456744 Valid Loss: 0.10075544193387032 Valid Accuracy: 0.9594006240367889\n",
      "198/350 Train Loss: 0.08411572175100446 Valid Loss: 0.12011070176959038 Valid Accuracy: 0.9433913826942444\n",
      "199/350 Train Loss: 0.08456733698646228 Valid Loss: 0.15644575655460358 Valid Accuracy: 0.9203381240367889\n",
      "200/350 Train Loss: 0.08764670222687225 Valid Loss: 0.17048833519220352 Valid Accuracy: 0.9523565471172333\n",
      "201/350 Train Loss: 0.09089519766469796 Valid Loss: 0.13673703372478485 Valid Accuracy: 0.9363473355770111\n",
      "202/350 Train Loss: 0.09726970332364242 Valid Loss: 0.11207838356494904 Valid Accuracy: 0.9363473355770111\n",
      "203/350 Train Loss: 0.08630953232447307 Valid Loss: 0.12041261792182922 Valid Accuracy: 0.9527407884597778\n",
      "204/350 Train Loss: 0.07917543997367223 Valid Loss: 0.12039078399538994 Valid Accuracy: 0.9445440471172333\n",
      "205/350 Train Loss: 0.07758235896471888 Valid Loss: 0.13915514573454857 Valid Accuracy: 0.9441598355770111\n",
      "206/350 Train Loss: 0.06974509575714667 Valid Loss: 0.13004830852150917 Valid Accuracy: 0.9609375\n",
      "207/350 Train Loss: 0.07411476193616788 Valid Loss: 0.16425127908587456 Valid Accuracy: 0.9285348355770111\n",
      "208/350 Train Loss: 0.07311020667354266 Valid Loss: 0.14265846461057663 Valid Accuracy: 0.9441598355770111\n",
      "209/350 Train Loss: 0.14969788367549577 Valid Loss: 0.16942667961120605 Valid Accuracy: 0.9359631240367889\n",
      "210/350 Train Loss: 0.11606219969689846 Valid Loss: 0.1726512759923935 Valid Accuracy: 0.9363473355770111\n",
      "211/350 Train Loss: 0.09888903734584649 Valid Loss: 0.12513649463653564 Valid Accuracy: 0.9441598355770111\n",
      "212/350 Train Loss: 0.06678135739639401 Valid Loss: 0.1348453164100647 Valid Accuracy: 0.9359631240367889\n",
      "213/350 Train Loss: 0.07740684086456895 Valid Loss: 0.13957476988434792 Valid Accuracy: 0.9515881240367889\n",
      "214/350 Train Loss: 0.09683539035419624 Valid Loss: 0.1445116624236107 Valid Accuracy: 0.9605532884597778\n",
      "215/350 Train Loss: 0.09194110706448555 Valid Loss: 0.1082933135330677 Valid Accuracy: 0.9527407884597778\n",
      "216/350 Train Loss: 0.07061151477197807 Valid Loss: 0.11594800092279911 Valid Accuracy: 0.9512038826942444\n",
      "217/350 Train Loss: 0.08719730998078982 Valid Loss: 0.10797053389251232 Valid Accuracy: 0.9683657884597778\n",
      "218/350 Train Loss: 0.07347032676140468 Valid Loss: 0.14230748265981674 Valid Accuracy: 0.9363473355770111\n",
      "219/350 Train Loss: 0.08002313102285068 Valid Loss: 0.14816424623131752 Valid Accuracy: 0.9359631240367889\n",
      "220/350 Train Loss: 0.08898643978560965 Valid Loss: 0.1455715075135231 Valid Accuracy: 0.9441598355770111\n",
      "221/350 Train Loss: 0.09470130642876029 Valid Loss: 0.15866629034280777 Valid Accuracy: 0.9363473355770111\n",
      "222/350 Train Loss: 0.09392091919047137 Valid Loss: 0.12920445948839188 Valid Accuracy: 0.9515881240367889\n",
      "223/350 Train Loss: 0.07854788269226749 Valid Loss: 0.14375397562980652 Valid Accuracy: 0.9285348355770111\n",
      "224/350 Train Loss: 0.07183279562741518 Valid Loss: 0.13686124235391617 Valid Accuracy: 0.9433913826942444\n",
      "225/350 Train Loss: 0.092227878049016 Valid Loss: 0.1315331868827343 Valid Accuracy: 0.9281506240367889\n",
      "226/350 Train Loss: 0.07456502970308065 Valid Loss: 0.12611326947808266 Valid Accuracy: 0.9199538826942444\n",
      "227/350 Train Loss: 0.06839330463359754 Valid Loss: 0.13637610524892807 Valid Accuracy: 0.9445440471172333\n",
      "228/350 Train Loss: 0.08866811450570822 Valid Loss: 0.12800057977437973 Valid Accuracy: 0.9445440471172333\n",
      "229/350 Train Loss: 0.11680319129178922 Valid Loss: 0.1323084942996502 Valid Accuracy: 0.9523565471172333\n",
      "230/350 Train Loss: 0.086854574115326 Valid Loss: 0.17821265757083893 Valid Accuracy: 0.9355788826942444\n",
      "231/350 Train Loss: 0.09675602242350578 Valid Loss: 0.13697972893714905 Valid Accuracy: 0.9359631240367889\n",
      "232/350 Train Loss: 0.07673723592112462 Valid Loss: 0.12199001014232635 Valid Accuracy: 0.9527407884597778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/350 Train Loss: 0.06990063407768805 Valid Loss: 0.18455137312412262 Valid Accuracy: 0.9433913826942444\n",
      "234/350 Train Loss: 0.07836703521509965 Valid Loss: 0.13946305960416794 Valid Accuracy: 0.9519723355770111\n",
      "235/350 Train Loss: 0.05871496396139264 Valid Loss: 0.13678675144910812 Valid Accuracy: 0.9519723355770111\n",
      "236/350 Train Loss: 0.056668292420605816 Valid Loss: 0.11586075276136398 Valid Accuracy: 0.9367315471172333\n",
      "237/350 Train Loss: 0.08956005377694964 Valid Loss: 0.12482553720474243 Valid Accuracy: 0.9355788826942444\n",
      "238/350 Train Loss: 0.08668250233555834 Valid Loss: 0.1757207065820694 Valid Accuracy: 0.9601690471172333\n",
      "239/350 Train Loss: 0.08862721702704827 Valid Loss: 0.17292630672454834 Valid Accuracy: 0.9437756240367889\n",
      "240/350 Train Loss: 0.08153937539706628 Valid Loss: 0.13716379180550575 Valid Accuracy: 0.9433913826942444\n",
      "241/350 Train Loss: 0.06721753456319372 Valid Loss: 0.18887213617563248 Valid Accuracy: 0.9363473355770111\n",
      "242/350 Train Loss: 0.0924764530112346 Valid Loss: 0.13117549568414688 Valid Accuracy: 0.9515881240367889\n",
      "243/350 Train Loss: 0.10087456119557221 Valid Loss: 0.10074564628303051 Valid Accuracy: 0.9355788826942444\n",
      "244/350 Train Loss: 0.09650545194745064 Valid Loss: 0.11455809324979782 Valid Accuracy: 0.9441598355770111\n",
      "245/350 Train Loss: 0.06589353441571195 Valid Loss: 0.10904315300285816 Valid Accuracy: 0.9605532884597778\n",
      "246/350 Train Loss: 0.07632747199386358 Valid Loss: 0.12730208225548267 Valid Accuracy: 0.9445440471172333\n",
      "247/350 Train Loss: 0.10330786711225907 Valid Loss: 0.14675677567720413 Valid Accuracy: 0.9515881240367889\n",
      "248/350 Train Loss: 0.10049727807442348 Valid Loss: 0.1374988853931427 Valid Accuracy: 0.9355788826942444\n",
      "249/350 Train Loss: 0.08116290625184774 Valid Loss: 0.15385971684008837 Valid Accuracy: 0.9430071711540222\n",
      "250/350 Train Loss: 0.07564181198055546 Valid Loss: 0.15969744324684143 Valid Accuracy: 0.9519723355770111\n",
      "251/350 Train Loss: 0.0735355409172674 Valid Loss: 0.15999006107449532 Valid Accuracy: 0.9367315471172333\n",
      "252/350 Train Loss: 0.07374577162166436 Valid Loss: 0.15195287019014359 Valid Accuracy: 0.9515881240367889\n",
      "253/350 Train Loss: 0.07441276855145891 Valid Loss: 0.13847924396395683 Valid Accuracy: 0.9441598355770111\n",
      "254/350 Train Loss: 0.08706613769754767 Valid Loss: 0.11468528397381306 Valid Accuracy: 0.9675973355770111\n",
      "255/350 Train Loss: 0.0694666161822776 Valid Loss: 0.1884082406759262 Valid Accuracy: 0.9359631240367889\n",
      "256/350 Train Loss: 0.08113822434097528 Valid Loss: 0.2349497601389885 Valid Accuracy: 0.9109887182712555\n",
      "257/350 Train Loss: 0.08649976225569844 Valid Loss: 0.22777975350618362 Valid Accuracy: 0.9285348355770111\n",
      "258/350 Train Loss: 0.10545207482452194 Valid Loss: 0.12369820196181536 Valid Accuracy: 0.9426229596138\n",
      "259/350 Train Loss: 0.09019207240392764 Valid Loss: 0.1102294884622097 Valid Accuracy: 0.9519723355770111\n",
      "260/350 Train Loss: 0.07681867387145758 Valid Loss: 0.13669641315937042 Valid Accuracy: 0.9441598355770111\n",
      "261/350 Train Loss: 0.10347361831615369 Valid Loss: 0.17190731316804886 Valid Accuracy: 0.9285348355770111\n",
      "262/350 Train Loss: 0.07787351418907444 Valid Loss: 0.189466193318367 Valid Accuracy: 0.9289190471172333\n",
      "263/350 Train Loss: 0.0664664963260293 Valid Loss: 0.19678036123514175 Valid Accuracy: 0.9359631240367889\n",
      "264/350 Train Loss: 0.07642643076057236 Valid Loss: 0.14907262101769447 Valid Accuracy: 0.9515881240367889\n",
      "265/350 Train Loss: 0.07486728904768825 Valid Loss: 0.12179145961999893 Valid Accuracy: 0.9515881240367889\n",
      "266/350 Train Loss: 0.07203884112338226 Valid Loss: 0.23630762100219727 Valid Accuracy: 0.9195696711540222\n",
      "267/350 Train Loss: 0.09376786354308327 Valid Loss: 0.14032390713691711 Valid Accuracy: 0.9433913826942444\n",
      "268/350 Train Loss: 0.0829403749667108 Valid Loss: 0.1072768084704876 Valid Accuracy: 0.9527407884597778\n",
      "269/350 Train Loss: 0.06568355951458216 Valid Loss: 0.1200670562684536 Valid Accuracy: 0.9519723355770111\n",
      "270/350 Train Loss: 0.08039890260746081 Valid Loss: 0.13400545343756676 Valid Accuracy: 0.9367315471172333\n",
      "271/350 Train Loss: 0.07081865162278216 Valid Loss: 0.11847499385476112 Valid Accuracy: 0.9433913826942444\n",
      "272/350 Train Loss: 0.07038201577961445 Valid Loss: 0.13190607354044914 Valid Accuracy: 0.9441598355770111\n",
      "273/350 Train Loss: 0.07241371211906274 Valid Loss: 0.14739423617720604 Valid Accuracy: 0.9367315471172333\n",
      "274/350 Train Loss: 0.06641954462975264 Valid Loss: 0.12914888933300972 Valid Accuracy: 0.9515881240367889\n",
      "275/350 Train Loss: 0.05587830161675811 Valid Loss: 0.14747891575098038 Valid Accuracy: 0.9277663826942444\n",
      "276/350 Train Loss: 0.06803555119161804 Valid Loss: 0.14565343037247658 Valid Accuracy: 0.9512038826942444\n",
      "277/350 Train Loss: 0.07223998258511226 Valid Loss: 0.1457553505897522 Valid Accuracy: 0.9363473355770111\n",
      "278/350 Train Loss: 0.10778128495439887 Valid Loss: 0.22045498341321945 Valid Accuracy: 0.9027920067310333\n",
      "279/350 Train Loss: 0.09154106335093577 Valid Loss: 0.1637134775519371 Valid Accuracy: 0.9355788826942444\n",
      "280/350 Train Loss: 0.07575337029993534 Valid Loss: 0.1461980938911438 Valid Accuracy: 0.9363473355770111\n",
      "281/350 Train Loss: 0.07157098843405645 Valid Loss: 0.15881971269845963 Valid Accuracy: 0.9597848355770111\n",
      "282/350 Train Loss: 0.07203309253479044 Valid Loss: 0.15974979475140572 Valid Accuracy: 0.9273821711540222\n",
      "283/350 Train Loss: 0.05602561465154091 Valid Loss: 0.14523974061012268 Valid Accuracy: 0.9437756240367889\n",
      "284/350 Train Loss: 0.052236225455999374 Valid Loss: 0.2062656506896019 Valid Accuracy: 0.9277663826942444\n",
      "285/350 Train Loss: 0.05409799019495646 Valid Loss: 0.14795522764325142 Valid Accuracy: 0.9519723355770111\n",
      "286/350 Train Loss: 0.06281042440483968 Valid Loss: 0.16854065656661987 Valid Accuracy: 0.9445440471172333\n",
      "287/350 Train Loss: 0.07924654167921592 Valid Loss: 0.17786428332328796 Valid Accuracy: 0.9441598355770111\n",
      "288/350 Train Loss: 0.077135787345469 Valid Loss: 0.20439907163381577 Valid Accuracy: 0.9437756240367889\n",
      "289/350 Train Loss: 0.05394027987495065 Valid Loss: 0.1732301041483879 Valid Accuracy: 0.9437756240367889\n",
      "290/350 Train Loss: 0.06193747806052367 Valid Loss: 0.21666757762432098 Valid Accuracy: 0.9430071711540222\n",
      "291/350 Train Loss: 0.05720332885781924 Valid Loss: 0.13710201159119606 Valid Accuracy: 0.9519723355770111\n",
      "292/350 Train Loss: 0.055828535774101816 Valid Loss: 0.15363863110542297 Valid Accuracy: 0.9594006240367889\n",
      "293/350 Train Loss: 0.06243870003769795 Valid Loss: 0.18669339269399643 Valid Accuracy: 0.9601690471172333\n",
      "294/350 Train Loss: 0.06951204920187593 Valid Loss: 0.14435907825827599 Valid Accuracy: 0.9527407884597778\n",
      "295/350 Train Loss: 0.0507809283832709 Valid Loss: 0.12657924741506577 Valid Accuracy: 0.9515881240367889\n",
      "296/350 Train Loss: 0.05617762434606751 Valid Loss: 0.17053105309605598 Valid Accuracy: 0.9519723355770111\n",
      "297/350 Train Loss: 0.0538499296332399 Valid Loss: 0.18410807102918625 Valid Accuracy: 0.9355788826942444\n",
      "298/350 Train Loss: 0.06480176855499546 Valid Loss: 0.2725075073540211 Valid Accuracy: 0.9113729596138\n",
      "299/350 Train Loss: 0.11736391531303525 Valid Loss: 0.31572671234607697 Valid Accuracy: 0.9047131240367889\n",
      "300/350 Train Loss: 0.15377146719644466 Valid Loss: 0.2424907386302948 Valid Accuracy: 0.8559170067310333\n",
      "301/350 Train Loss: 0.13795172205815712 Valid Loss: 0.14461429324001074 Valid Accuracy: 0.9609375\n",
      "302/350 Train Loss: 0.10111433096850912 Valid Loss: 0.14372559264302254 Valid Accuracy: 0.9351946711540222\n",
      "303/350 Train Loss: 0.12505136740704378 Valid Loss: 0.15412798523902893 Valid Accuracy: 0.9351946711540222\n",
      "304/350 Train Loss: 0.09976906422525644 Valid Loss: 0.19478992745280266 Valid Accuracy: 0.9453125\n",
      "305/350 Train Loss: 0.0809533732632796 Valid Loss: 0.19757677614688873 Valid Accuracy: 0.9363473355770111\n",
      "306/350 Train Loss: 0.07648800639435649 Valid Loss: 0.1371021270751953 Valid Accuracy: 0.9359631240367889\n",
      "307/350 Train Loss: 0.07079737121239305 Valid Loss: 0.21334107592701912 Valid Accuracy: 0.9359631240367889\n",
      "308/350 Train Loss: 0.07429343027373154 Valid Loss: 0.14995481818914413 Valid Accuracy: 0.9437756240367889\n",
      "309/350 Train Loss: 0.0650719318849345 Valid Loss: 0.14438821002840996 Valid Accuracy: 0.9441598355770111\n",
      "310/350 Train Loss: 0.056254140411814056 Valid Loss: 0.14506058394908905 Valid Accuracy: 0.9519723355770111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/350 Train Loss: 0.05784694701045131 Valid Loss: 0.21839850023388863 Valid Accuracy: 0.9367315471172333\n",
      "312/350 Train Loss: 0.050779788910100855 Valid Loss: 0.15931204706430435 Valid Accuracy: 0.9445440471172333\n",
      "313/350 Train Loss: 0.0559018769611915 Valid Loss: 0.21554845571517944 Valid Accuracy: 0.9437756240367889\n",
      "314/350 Train Loss: 0.049760017388810716 Valid Loss: 0.18184523284435272 Valid Accuracy: 0.9363473355770111\n",
      "315/350 Train Loss: 0.06147096569960316 Valid Loss: 0.15725567936897278 Valid Accuracy: 0.9519723355770111\n",
      "316/350 Train Loss: 0.05673135172886153 Valid Loss: 0.23331859707832336 Valid Accuracy: 0.9285348355770111\n",
      "317/350 Train Loss: 0.059771803518136345 Valid Loss: 0.1542081981897354 Valid Accuracy: 0.9433913826942444\n",
      "318/350 Train Loss: 0.053648036594192185 Valid Loss: 0.18647852540016174 Valid Accuracy: 0.9437756240367889\n",
      "319/350 Train Loss: 0.05687456523689131 Valid Loss: 0.1706365868449211 Valid Accuracy: 0.9512038826942444\n",
      "320/350 Train Loss: 0.07291060375670592 Valid Loss: 0.1489918828010559 Valid Accuracy: 0.9601690471172333\n",
      "321/350 Train Loss: 0.06057442305609584 Valid Loss: 0.17335905507206917 Valid Accuracy: 0.9273821711540222\n",
      "322/350 Train Loss: 0.06949710093128185 Valid Loss: 0.27821721136569977 Valid Accuracy: 0.9121413826942444\n",
      "323/350 Train Loss: 0.08944444327304761 Valid Loss: 0.2271937131881714 Valid Accuracy: 0.9445440471172333\n",
      "324/350 Train Loss: 0.1003194044654568 Valid Loss: 0.20892417430877686 Valid Accuracy: 0.9437756240367889\n",
      "325/350 Train Loss: 0.07522601525609691 Valid Loss: 0.12789097800850868 Valid Accuracy: 0.9601690471172333\n",
      "326/350 Train Loss: 0.07224255970989664 Valid Loss: 0.1372324302792549 Valid Accuracy: 0.9445440471172333\n",
      "327/350 Train Loss: 0.06036833627149463 Valid Loss: 0.1409924509935081 Valid Accuracy: 0.9453125\n",
      "328/350 Train Loss: 0.054725117360552154 Valid Loss: 0.1468867352232337 Valid Accuracy: 0.9527407884597778\n",
      "329/350 Train Loss: 0.05029231263324618 Valid Loss: 0.22720203548669815 Valid Accuracy: 0.9441598355770111\n",
      "330/350 Train Loss: 0.10327946674078703 Valid Loss: 0.20722000673413277 Valid Accuracy: 0.9211065471172333\n",
      "331/350 Train Loss: 0.08746756023416917 Valid Loss: 0.15001524984836578 Valid Accuracy: 0.9285348355770111\n",
      "332/350 Train Loss: 0.07282941105465095 Valid Loss: 0.16843054443597794 Valid Accuracy: 0.9355788826942444\n",
      "333/350 Train Loss: 0.06821730763961871 Valid Loss: 0.13260262832045555 Valid Accuracy: 0.9601690471172333\n",
      "334/350 Train Loss: 0.06014894383649031 Valid Loss: 0.16712567955255508 Valid Accuracy: 0.9355788826942444\n",
      "335/350 Train Loss: 0.05375994167601069 Valid Loss: 0.13448778167366982 Valid Accuracy: 0.9441598355770111\n",
      "336/350 Train Loss: 0.05111233346785108 Valid Loss: 0.16402443870902061 Valid Accuracy: 0.9519723355770111\n",
      "337/350 Train Loss: 0.050733720883727074 Valid Loss: 0.1496465653181076 Valid Accuracy: 0.9519723355770111\n",
      "338/350 Train Loss: 0.05534665938466787 Valid Loss: 0.19950145110487938 Valid Accuracy: 0.9351946711540222\n",
      "339/350 Train Loss: 0.05165508560215434 Valid Loss: 0.16424064710736275 Valid Accuracy: 0.9445440471172333\n",
      "340/350 Train Loss: 0.06655884208157659 Valid Loss: 0.17331992834806442 Valid Accuracy: 0.9433913826942444\n",
      "341/350 Train Loss: 0.07236103309939305 Valid Loss: 0.1618652269244194 Valid Accuracy: 0.9515881240367889\n",
      "342/350 Train Loss: 0.06667989337195952 Valid Loss: 0.17114277929067612 Valid Accuracy: 0.9523565471172333\n",
      "343/350 Train Loss: 0.056779543248315655 Valid Loss: 0.17891372740268707 Valid Accuracy: 0.9594006240367889\n",
      "344/350 Train Loss: 0.05199797575672468 Valid Loss: 0.21597615629434586 Valid Accuracy: 0.9441598355770111\n",
      "345/350 Train Loss: 0.0866869711317122 Valid Loss: 0.172409288585186 Valid Accuracy: 0.9519723355770111\n",
      "346/350 Train Loss: 0.07610809624505539 Valid Loss: 0.17211616784334183 Valid Accuracy: 0.9441598355770111\n",
      "347/350 Train Loss: 0.09594684202844898 Valid Loss: 0.25600556284189224 Valid Accuracy: 0.9359631240367889\n",
      "348/350 Train Loss: 0.09096001259361704 Valid Loss: 0.30490146577358246 Valid Accuracy: 0.9285348355770111\n",
      "349/350 Train Loss: 0.10499739522735278 Valid Loss: 0.15418988093733788 Valid Accuracy: 0.9359631240367889\n",
      "350/350 Train Loss: 0.09301931054020922 Valid Loss: 0.12771936878561974 Valid Accuracy: 0.9355788826942444\n",
      "min loss: 0.08679206669330597\n",
      "Training Time is: 38.38370323181152 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0a72751d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "accuracy = train(net_predict,train_loader,valid_loader,350)\n",
    "print('Training Time is:',time()-start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd12940f9b0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(accuracy,color='b',label='accuracy')\n",
    "#plt.title('Accuracy_Trend')\n",
    "#plt.xlabel('Epoches')\n",
    "#plt.ylabel('Accuracy')\n",
    "plt.plot(accuracy,color='b',label='准确率')\n",
    "plt.title('准确率变化')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('准确率')\n",
    "plt.legend()\n",
    "plt.savefig('behavior_image/accuracy1.svg',dpi=300)\n",
    "plt.savefig('behavior_image/accuracy1.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,test_loader):\n",
    "    valid_loss = []\n",
    "    accuracies_ = []\n",
    "    net.eval()\n",
    "    class_correct = np.zeros(3)\n",
    "    class_total = np.zeros(3)\n",
    "    classes = ['left','keep','right']\n",
    "    for x,y in test_loader:\n",
    "        if use_cuda:\n",
    "            x,y = x.cuda(),y.cuda()\n",
    "        out = net(x)\n",
    "        loss = criterion(out,y)\n",
    "        _,class_ = torch.max(out,dim=1)\n",
    "        equal = class_ == y.view(class_.shape)\n",
    "        for i in range(y.shape[0]):\n",
    "            label = y.data[i].item()\n",
    "            class_correct[label] += equal[i].item()\n",
    "            class_total[label] += 1\n",
    "        accuracy = torch.mean(equal.type(torch.FloatTensor)).item()\n",
    "        accuracies_.append(accuracy)\n",
    "        \n",
    "        valid_loss.append(loss.item())\n",
    "    print('Test Loss: {:.6f}\\n'.format(loss.item()))\n",
    "    for i in range(3):\n",
    "        if class_total[i]>0:\n",
    "            print('Test Accuracy of {}:{:.4f}({}/{})'.format(classes[i],100*class_correct[i]/class_total[i],\n",
    "                 int(np.sum(class_correct[i])),\n",
    "                 int(np.sum(class_total[i]))))\n",
    "        else:\n",
    "            print('Test Accuracy of {}:N/A(no examples)'.format(classes[i]))\n",
    "    print('Test Accuracy(Overall):{:.4f} ({}/{})'.format(100*np.sum(class_correct)/np.sum(class_total),\n",
    "                                                    int(np.sum(class_correct)),\n",
    "                                                    int(np.sum(class_total))))\n",
    "    print('Test Loss:',np.mean(valid_loss),'Test Accuracy:',np.mean(accuracies_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.067040\n",
      "\n",
      "Test Accuracy of left:97.6190(41/42)\n",
      "Test Accuracy of keep:95.9184(47/49)\n",
      "Test Accuracy of right:97.0588(33/34)\n",
      "Test Accuracy(Overall):96.8000 (121/125)\n",
      "Test Loss: 0.07946557179093361 Test Accuracy: 0.9679815471172333\n"
     ]
    }
   ],
   "source": [
    "net = trajectory_predict(X_train.shape[1],256,3)\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "net.load_state_dict(torch.load('model/net_bahavior_loss_min_mlp.pth'))\n",
    "test(net,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 2.3190975189208984\n",
      "tensor([[-26.8236,  -8.0053,  22.8455]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[-26.8236,  -8.0053,  22.8455]], device='cuda:0',\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n"
     ]
    }
   ],
   "source": [
    "net.cuda().eval()\n",
    "\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "#example = torch.rand(1, 4)\n",
    "example = torch.ones(1, 4).cuda()\n",
    "t = time()\n",
    "out = net(example)\n",
    "print('time',(time()-t)*1000)\n",
    "print(out)\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_script_module = torch.jit.trace(net, example)\n",
    "traced_script_module.save(\"model_cuda.pt\")\n",
    "out = traced_script_module(example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
