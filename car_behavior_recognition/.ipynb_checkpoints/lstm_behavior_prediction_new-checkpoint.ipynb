{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34.76807295828066, 0.8329138124321809, 8.206638634642987, -0.9989611424577053], [39.70847572869826, 7.605497519811325, 8.159740556853095, 0.4016710566825881]] [0, 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "def labeltoint(label):\n",
    "    if label == 'left':\n",
    "        label = 0\n",
    "    if label == 'keep':\n",
    "        label = 1\n",
    "    if label == 'right':\n",
    "        label = 2\n",
    "    return label\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open('data1/train.json', 'r') as f:\n",
    "    j = json.load(f)\n",
    "  #  print(j.keys())\n",
    "    X_train = j['states']\n",
    "    Y_train = j['labels']\n",
    "    for i in range(len(Y_train)):\n",
    "        Y_train[i] = labeltoint(Y_train[i])\n",
    "  #  print(Y_train)\n",
    "\n",
    "with open('data1/test.json', 'r') as f:\n",
    "    j = json.load(f)\n",
    "    X_test = j['states']\n",
    "    Y_test = j['labels']\n",
    "    for i in range(len(Y_test)):\n",
    "        Y_test[i] = labeltoint(Y_test[i])\n",
    "print(X_train[:2],Y_train[:2])\n",
    "split_frac = 0.8\n",
    "X_train, Y_train, X_test, Y_test = np.array(X_train).astype(np.float32), np.array(Y_train).astype(np.long), np.array(\n",
    "    X_test).astype(np.float32), np.array(Y_test).astype(np.long)\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "val_x, test_x = X_test[:len(X_test) // 2], X_test[len(X_test) // 2:]\n",
    "val_y, test_y = Y_test[:len(Y_test) // 2], Y_test[len(Y_test) // 2:]\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy((X_train)), torch.from_numpy(Y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        # define an RNN with specified parameters\n",
    "        # batch_first means that the first dim of the input and output will be the batch_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers,dropout=0, batch_first=True)\n",
    "        \n",
    "        # last, fully-connected layer\n",
    "        self.fc1 = nn.Linear(hidden_dim,hidden_dim*2)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x, hidden):\n",
    "        # x (batch_size, seq_length, input_size)\n",
    "        # hidden (n_layers, batch_size, hidden_dim)\n",
    "        # r_out (batch_size, time_step, hidden_size)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # get RNN outputs\n",
    "        r_out, hidden = self.lstm(x, hidden)\n",
    "       # print(r_out.shape,hidden[0].shape)\n",
    "        # shape output to be (batch_size*seq_length, hidden_dim)\n",
    "        r_out = r_out.contiguous().view(-1, self.hidden_dim)  \n",
    "        \n",
    "        # get final output \n",
    "        output = self.fc1(r_out)\n",
    "        output = self.fc(output)\n",
    "       # output = output.view(batch_size,-1,3)\n",
    "     #   output = output[:,-1]\n",
    "      #  output = self.softmax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_on_gpu = True\n",
    "else:\n",
    "    train_on_gpu = False\n",
    "\n",
    "\n",
    "# In[8]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,epochs,train_loader,valid_loader,clip,lr = 0.0002):\n",
    "    # train for some number of epochs\n",
    "    # loss and optimization functions\n",
    "\n",
    "    loss_min = np.inf\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    counter = 0\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    accuracies_e = []\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        net.train()\n",
    "        # batch loop\n",
    "        train_loss = []\n",
    "        for inputs, labels in train_loader:\n",
    "            h = net.init_hidden(inputs.shape[0])\n",
    "            h = tuple([each.data for each in h])\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            if (train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "          #  print(output.shape,labels.shape)\n",
    "           # print(output[:1])\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            # loss stats\n",
    "                # Get validation loss\n",
    "\n",
    "        val_losses = []\n",
    "        net.eval()\n",
    "        accuracies = []\n",
    "        for inputs, labels in valid_loader:\n",
    "            val_h = net.init_hidden(inputs.shape[0])\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                   # val_h = tuple([each.data for each in val_h])\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "            if (train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            output, val_h = net(inputs, val_h)\n",
    "\n",
    "            val_loss = criterion(output, labels)\n",
    "            _, class_ = torch.max(output, dim=1)\n",
    "            equal = class_ == labels.view(class_.shape)\n",
    "            accuracy = torch.mean(equal.type(torch.FloatTensor)).item()\n",
    "            val_losses.append(val_loss.item())\n",
    "            accuracies.append(accuracy)\n",
    "                \n",
    "\n",
    "        net.train()\n",
    "        losses_train.append(np.mean(train_loss))\n",
    "        losses_valid.append(np.mean(val_losses))\n",
    "        accuracies_e.append(np.mean(np.mean(accuracies)))\n",
    "        print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "                      \"Loss: {}...\".format(np.mean(train_loss)),\n",
    "                      \"Val Loss: {}...\".format(np.mean(val_losses)),\n",
    "                      \"val accuracy:{}.\".format(np.mean(accuracies))\n",
    "                     )\n",
    "        if np.mean(val_losses) < loss_min:\n",
    "            print('Val loss decreased...')\n",
    "            torch.save(net.state_dict(),'model/lstm_behavior_prediction_modefy.pt')\n",
    "            loss_min = np.mean(val_losses)\n",
    "    \n",
    "    print('min loss',loss_min)\n",
    "   # plt.plot(losses_train,color='r',label='train_loss')\n",
    "    #plt.plot(losses_valid,color='g',label='valid_loss')\n",
    "    plt.plot(losses_train,color='r',label='训练损失')\n",
    "    plt.plot(losses_valid,color='g',label='验证损失')\n",
    "    #plt.title('Loss_Trend')\n",
    "    #plt.xlabel('Epoches')\n",
    "    #plt.ylabel('Loss')\n",
    "    plt.title('损失变化')\n",
    "    plt.xlabel('迭代次数')\n",
    "    plt.ylabel('损失大小')\n",
    "    plt.legend()\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.savefig('behavior_image/loss_lstm_behavior_modefy1.svg',dpi=300)\n",
    "    plt.savefig('behavior_image/loss_lstm_behavior_modefy1.png',dpi=300)\n",
    "    return accuracies_e\n",
    "                            \n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def test(net,test_loader):\n",
    "    # Get test data loss and accuracy\n",
    "    lr = 0.001\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_losses = []  # track loss\n",
    "    accuracies = []\n",
    "    net.eval()\n",
    "    # iterate over test data\n",
    "    class_correct = np.zeros(3)\n",
    "    class_total = np.zeros(3)\n",
    "    classes = ['left','keep','right']\n",
    "    for inputs, y in test_loader:\n",
    "        h = net.init_hidden(inputs.shape[0])\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            inputs, y = inputs.cuda(), y.cuda()\n",
    "        # get predicted outputs\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate loss\n",
    "        test_loss = criterion(output, y)\n",
    "        _, class_ = torch.max(output, dim=1)\n",
    "        equal = class_ == y.view(class_.shape)\n",
    "        for i in range(y.shape[0]):\n",
    "            label = y.data[i].item()\n",
    "            class_correct[label] += equal[i].item()\n",
    "            class_total[label] += 1\n",
    "        accuracy = torch.mean(equal.type(torch.FloatTensor)).item()\n",
    "        test_losses.append(test_loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # -- stats! -- ##\n",
    "    # avg test loss\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss.item()))\n",
    "    for i in range(3):\n",
    "        if class_total[i]>0:\n",
    "            print('Test Accuracy of {}:{:.4f}({}/{})'.format(classes[i],100*class_correct[i]/class_total[i],\n",
    "                 int(np.sum(class_correct[i])),\n",
    "                 int(np.sum(class_total[i]))))\n",
    "        else:\n",
    "            print('Test Accuracy of {}:N/A(no examples)'.format(classes[i]))\n",
    "    print('Test Accuracy(Overall):{:.4f} ({}/{})'.format(100*np.sum(class_correct)/np.sum(class_total),\n",
    "                                                    int(np.sum(class_correct)),\n",
    "                                                    int(np.sum(class_total))))\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)),'Test Accuracy:{}'.format(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/350... Loss: 1.084535390138626... Val Loss: 1.0820871591567993... val accuracy:0.40176741778850555.\n",
      "Val loss decreased...\n",
      "Epoch: 2/350... Loss: 1.064515729745229... Val Loss: 1.0615050196647644... val accuracy:0.39946208894252777.\n",
      "Val loss decreased...\n",
      "Epoch: 3/350... Loss: 1.0028271973133087... Val Loss: 0.9440722167491913... val accuracy:0.5286885201931.\n",
      "Val loss decreased...\n",
      "Epoch: 4/350... Loss: 0.8479504336913427... Val Loss: 0.7440188527107239... val accuracy:0.5942623019218445.\n",
      "Val loss decreased...\n",
      "Epoch: 5/350... Loss: 0.6478232542673746... Val Loss: 0.512584999203682... val accuracy:0.8004610538482666.\n",
      "Val loss decreased...\n",
      "Epoch: 6/350... Loss: 0.4437876765926679... Val Loss: 0.338961660861969... val accuracy:0.8641137182712555.\n",
      "Val loss decreased...\n",
      "Epoch: 7/350... Loss: 0.3554922466476758... Val Loss: 0.2777582108974457... val accuracy:0.8566854596138.\n",
      "Val loss decreased...\n",
      "Epoch: 8/350... Loss: 0.29326172669728595... Val Loss: 0.270074762403965... val accuracy:0.8972848355770111.\n",
      "Val loss decreased...\n",
      "Epoch: 9/350... Loss: 0.2862621732056141... Val Loss: 0.280248299241066... val accuracy:0.8574538826942444.\n",
      "Epoch: 10/350... Loss: 0.2554924755046765... Val Loss: 0.2000565454363823... val accuracy:0.8957479596138.\n",
      "Val loss decreased...\n",
      "Epoch: 11/350... Loss: 0.24179637804627419... Val Loss: 0.18905533850193024... val accuracy:0.8805071711540222.\n",
      "Val loss decreased...\n",
      "Epoch: 12/350... Loss: 0.23839475959539413... Val Loss: 0.19209133088588715... val accuracy:0.9039446711540222.\n",
      "Epoch: 13/350... Loss: 0.21757691726088524... Val Loss: 0.1935914307832718... val accuracy:0.8969006240367889.\n",
      "Epoch: 14/350... Loss: 0.20859086265166601... Val Loss: 0.17880747467279434... val accuracy:0.9125256240367889.\n",
      "Val loss decreased...\n",
      "Epoch: 15/350... Loss: 0.19558192044496536... Val Loss: 0.20010197162628174... val accuracy:0.8890881240367889.\n",
      "Epoch: 16/350... Loss: 0.21406539405385652... Val Loss: 0.18309090286493301... val accuracy:0.9191854596138.\n",
      "Epoch: 17/350... Loss: 0.1873586749037107... Val Loss: 0.1579536534845829... val accuracy:0.8969006240367889.\n",
      "Val loss decreased...\n",
      "Epoch: 18/350... Loss: 0.17939186717073122... Val Loss: 0.14826001971960068... val accuracy:0.9273821711540222.\n",
      "Val loss decreased...\n",
      "Epoch: 19/350... Loss: 0.20381986225644746... Val Loss: 0.14927877485752106... val accuracy:0.9273821711540222.\n",
      "Epoch: 20/350... Loss: 0.18919255894919237... Val Loss: 0.14832988753914833... val accuracy:0.9207223355770111.\n",
      "Epoch: 21/350... Loss: 0.1833669158319632... Val Loss: 0.1672120913863182... val accuracy:0.9121413826942444.\n",
      "Epoch: 22/350... Loss: 0.18224416549007097... Val Loss: 0.1646197959780693... val accuracy:0.9195696711540222.\n",
      "Epoch: 23/350... Loss: 0.1799371223896742... Val Loss: 0.14650368690490723... val accuracy:0.9367315471172333.\n",
      "Val loss decreased...\n",
      "Epoch: 24/350... Loss: 0.18483754992485046... Val Loss: 0.14183389395475388... val accuracy:0.9277663826942444.\n",
      "Val loss decreased...\n",
      "Epoch: 25/350... Loss: 0.17561163815359274... Val Loss: 0.13716385513544083... val accuracy:0.9437756240367889.\n",
      "Val loss decreased...\n",
      "Epoch: 26/350... Loss: 0.16918726079165936... Val Loss: 0.16538238525390625... val accuracy:0.9351946711540222.\n",
      "Epoch: 27/350... Loss: 0.16540016109744707... Val Loss: 0.12210850417613983... val accuracy:0.9433913826942444.\n",
      "Val loss decreased...\n",
      "Epoch: 28/350... Loss: 0.1534612588584423... Val Loss: 0.16044194623827934... val accuracy:0.9207223355770111.\n",
      "Epoch: 29/350... Loss: 0.14528200527032217... Val Loss: 0.12830828875303268... val accuracy:0.9359631240367889.\n",
      "Epoch: 30/350... Loss: 0.1569331375261148... Val Loss: 0.16577715426683426... val accuracy:0.9043288826942444.\n",
      "Epoch: 31/350... Loss: 0.15417562425136566... Val Loss: 0.1250067502260208... val accuracy:0.9441598355770111.\n",
      "Epoch: 32/350... Loss: 0.15703514218330383... Val Loss: 0.12503864988684654... val accuracy:0.9441598355770111.\n",
      "Epoch: 33/350... Loss: 0.14666386134922504... Val Loss: 0.12824439629912376... val accuracy:0.9289190471172333.\n",
      "Epoch: 34/350... Loss: 0.1457906891591847... Val Loss: 0.13131758570671082... val accuracy:0.9441598355770111.\n",
      "Epoch: 35/350... Loss: 0.13643678463995457... Val Loss: 0.11069156974554062... val accuracy:0.9515881240367889.\n",
      "Val loss decreased...\n",
      "Epoch: 36/350... Loss: 0.15133492400248846... Val Loss: 0.12118900194764137... val accuracy:0.9519723355770111.\n",
      "Epoch: 37/350... Loss: 0.20638182138403258... Val Loss: 0.1486765220761299... val accuracy:0.9277663826942444.\n",
      "Epoch: 38/350... Loss: 0.22696125010649362... Val Loss: 0.17536571994423866... val accuracy:0.9129098355770111.\n",
      "Epoch: 39/350... Loss: 0.1978792573014895... Val Loss: 0.1476048342883587... val accuracy:0.9285348355770111.\n",
      "Epoch: 40/350... Loss: 0.1590787898749113... Val Loss: 0.1286255083978176... val accuracy:0.9437756240367889.\n",
      "Epoch: 41/350... Loss: 0.13173126553495726... Val Loss: 0.1062408946454525... val accuracy:0.9519723355770111.\n",
      "Val loss decreased...\n",
      "Epoch: 42/350... Loss: 0.12327841762453318... Val Loss: 0.11632927507162094... val accuracy:0.9437756240367889.\n",
      "Epoch: 43/350... Loss: 0.13728875511636338... Val Loss: 0.0993865318596363... val accuracy:0.9597848355770111.\n",
      "Val loss decreased...\n",
      "Epoch: 44/350... Loss: 0.12917261632780233... Val Loss: 0.09551427885890007... val accuracy:0.9601690471172333.\n",
      "Val loss decreased...\n",
      "Epoch: 45/350... Loss: 0.1337405468026797... Val Loss: 0.1166987232863903... val accuracy:0.9515881240367889.\n",
      "Epoch: 46/350... Loss: 0.12577861299117407... Val Loss: 0.0985918827354908... val accuracy:0.9601690471172333.\n",
      "Epoch: 47/350... Loss: 0.14518017383913198... Val Loss: 0.09819584712386131... val accuracy:0.9601690471172333.\n",
      "Epoch: 48/350... Loss: 0.14629564993083477... Val Loss: 0.12700984254479408... val accuracy:0.9285348355770111.\n",
      "Epoch: 49/350... Loss: 0.12928839214146137... Val Loss: 0.20926396548748016... val accuracy:0.8953637182712555.\n",
      "Epoch: 50/350... Loss: 0.13374951171378294... Val Loss: 0.09568611532449722... val accuracy:0.9523565471172333.\n",
      "Epoch: 51/350... Loss: 0.12202696005503337... Val Loss: 0.0865364819765091... val accuracy:0.9683657884597778.\n",
      "Val loss decreased...\n",
      "Epoch: 52/350... Loss: 0.14056645147502422... Val Loss: 0.09334810450673103... val accuracy:0.9675973355770111.\n",
      "Epoch: 53/350... Loss: 0.11428411491215229... Val Loss: 0.1256481222808361... val accuracy:0.9203381240367889.\n",
      "Epoch: 54/350... Loss: 0.13347471101830402... Val Loss: 0.12483105435967445... val accuracy:0.9519723355770111.\n",
      "Epoch: 55/350... Loss: 0.13118050837268433... Val Loss: 0.14222056418657303... val accuracy:0.9445440471172333.\n",
      "Epoch: 56/350... Loss: 0.16400356652836004... Val Loss: 0.17438631504774094... val accuracy:0.9277663826942444.\n",
      "Epoch: 57/350... Loss: 0.12220827614267667... Val Loss: 0.12900222837924957... val accuracy:0.9281506240367889.\n",
      "Epoch: 58/350... Loss: 0.12336091635127862... Val Loss: 0.20635005086660385... val accuracy:0.8957479596138.\n",
      "Epoch: 59/350... Loss: 0.17181957978755236... Val Loss: 0.09808768704533577... val accuracy:0.9515881240367889.\n",
      "Epoch: 60/350... Loss: 0.12446313661833604... Val Loss: 0.0917072743177414... val accuracy:0.9601690471172333.\n",
      "Epoch: 61/350... Loss: 0.12049105328818162... Val Loss: 0.08988144993782043... val accuracy:0.9515881240367889.\n",
      "Epoch: 62/350... Loss: 0.14329126135756573... Val Loss: 0.10511275380849838... val accuracy:0.9515881240367889.\n",
      "Epoch: 63/350... Loss: 0.13157483159253994... Val Loss: 0.08308220654726028... val accuracy:0.9597848355770111.\n",
      "Val loss decreased...\n",
      "Epoch: 64/350... Loss: 0.1224497032041351... Val Loss: 0.09076796099543571... val accuracy:0.9519723355770111.\n",
      "Epoch: 65/350... Loss: 0.12269958900287747... Val Loss: 0.15561377257108688... val accuracy:0.9281506240367889.\n",
      "Epoch: 66/350... Loss: 0.12454828309516112... Val Loss: 0.09181952476501465... val accuracy:0.9597848355770111.\n",
      "Epoch: 67/350... Loss: 0.11145893732706706... Val Loss: 0.11245039105415344... val accuracy:0.9512038826942444.\n",
      "Epoch: 68/350... Loss: 0.11242093611508608... Val Loss: 0.08391902223229408... val accuracy:0.9515881240367889.\n",
      "Epoch: 69/350... Loss: 0.10750547517091036... Val Loss: 0.08332276344299316... val accuracy:0.9359631240367889.\n",
      "Epoch: 70/350... Loss: 0.12259340627739827... Val Loss: 0.11841854080557823... val accuracy:0.9512038826942444.\n",
      "Epoch: 71/350... Loss: 0.12191640585660934... Val Loss: 0.10926984623074532... val accuracy:0.9363473355770111.\n",
      "Epoch: 72/350... Loss: 0.10317311249673367... Val Loss: 0.10397281125187874... val accuracy:0.9515881240367889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73/350... Loss: 0.11511307582259178... Val Loss: 0.1253782920539379... val accuracy:0.9441598355770111.\n",
      "Epoch: 74/350... Loss: 0.14136315075059733... Val Loss: 0.10892758145928383... val accuracy:0.9523565471172333.\n",
      "Epoch: 75/350... Loss: 0.10715106502175331... Val Loss: 0.1211902778595686... val accuracy:0.9508196711540222.\n",
      "Epoch: 76/350... Loss: 0.1031987772633632... Val Loss: 0.10989649966359138... val accuracy:0.9519723355770111.\n",
      "Epoch: 77/350... Loss: 0.11313290831943353... Val Loss: 0.08550184592604637... val accuracy:0.9515881240367889.\n",
      "Epoch: 78/350... Loss: 0.09945181384682655... Val Loss: 0.07342770509421825... val accuracy:0.9523565471172333.\n",
      "Val loss decreased...\n",
      "Epoch: 79/350... Loss: 0.10363995873679717... Val Loss: 0.09520301036536694... val accuracy:0.9675973355770111.\n",
      "Epoch: 80/350... Loss: 0.11347865732386708... Val Loss: 0.08358275145292282... val accuracy:0.9512038826942444.\n",
      "Epoch: 81/350... Loss: 0.12994586676359177... Val Loss: 0.13090913370251656... val accuracy:0.9367315471172333.\n",
      "Epoch: 82/350... Loss: 0.15357618468503156... Val Loss: 0.11265513487160206... val accuracy:0.9445440471172333.\n",
      "Epoch: 83/350... Loss: 0.15732414865245423... Val Loss: 0.0838997270911932... val accuracy:0.9594006240367889.\n",
      "Epoch: 84/350... Loss: 0.11197739963730176... Val Loss: 0.08866306021809578... val accuracy:0.9679815471172333.\n",
      "Epoch: 85/350... Loss: 0.1197770768776536... Val Loss: 0.11302710697054863... val accuracy:0.9519723355770111.\n",
      "Epoch: 86/350... Loss: 0.10384633811190724... Val Loss: 0.08915245160460472... val accuracy:0.9679815471172333.\n",
      "Epoch: 87/350... Loss: 0.10658276174217463... Val Loss: 0.1200069934129715... val accuracy:0.9433913826942444.\n",
      "Epoch: 88/350... Loss: 0.09822067804634571... Val Loss: 0.08846700191497803... val accuracy:0.9519723355770111.\n",
      "Epoch: 89/350... Loss: 0.14160897489637136... Val Loss: 0.1172131597995758... val accuracy:0.9519723355770111.\n",
      "Epoch: 90/350... Loss: 0.1393998625377814... Val Loss: 0.08106168918311596... val accuracy:0.9605532884597778.\n",
      "Epoch: 91/350... Loss: 0.10228149375567834... Val Loss: 0.09150228649377823... val accuracy:0.9597848355770111.\n",
      "Epoch: 92/350... Loss: 0.13327172584831715... Val Loss: 0.09403682872653008... val accuracy:0.9441598355770111.\n",
      "Epoch: 93/350... Loss: 0.12276353438695271... Val Loss: 0.10336177423596382... val accuracy:0.9515881240367889.\n",
      "Epoch: 94/350... Loss: 0.10716713685542345... Val Loss: 0.07671914622187614... val accuracy:0.9523565471172333.\n",
      "Epoch: 95/350... Loss: 0.10397239060451587... Val Loss: 0.11194902285933495... val accuracy:0.9441598355770111.\n",
      "Epoch: 96/350... Loss: 0.11395275468627612... Val Loss: 0.08166865073144436... val accuracy:0.9605532884597778.\n",
      "Epoch: 97/350... Loss: 0.09806749131530523... Val Loss: 0.08324047178030014... val accuracy:0.9523565471172333.\n",
      "Epoch: 98/350... Loss: 0.09711039035270612... Val Loss: 0.08643442578613758... val accuracy:0.9594006240367889.\n",
      "Epoch: 99/350... Loss: 0.1106628980487585... Val Loss: 0.11494194716215134... val accuracy:0.9601690471172333.\n",
      "Epoch: 100/350... Loss: 0.10463688957194488... Val Loss: 0.10345470905303955... val accuracy:0.9445440471172333.\n",
      "Epoch: 101/350... Loss: 0.09767737208555143... Val Loss: 0.2136562615633011... val accuracy:0.9195696711540222.\n",
      "Epoch: 102/350... Loss: 0.12213127625485261... Val Loss: 0.11895901709794998... val accuracy:0.9441598355770111.\n",
      "Epoch: 103/350... Loss: 0.09881833832090099... Val Loss: 0.10079596191644669... val accuracy:0.9515881240367889.\n",
      "Epoch: 104/350... Loss: 0.09545367816463113... Val Loss: 0.10139098018407822... val accuracy:0.9597848355770111.\n",
      "Epoch: 105/350... Loss: 0.09598554950207472... Val Loss: 0.09038911387324333... val accuracy:0.9601690471172333.\n",
      "Epoch: 106/350... Loss: 0.10376369208097458... Val Loss: 0.1175142303109169... val accuracy:0.9359631240367889.\n",
      "Epoch: 107/350... Loss: 0.12277738098055124... Val Loss: 0.11968223564326763... val accuracy:0.9590163826942444.\n",
      "Epoch: 108/350... Loss: 0.1363018803919355... Val Loss: 0.08898727409541607... val accuracy:0.96875.\n",
      "Epoch: 109/350... Loss: 0.10056611290201545... Val Loss: 0.06574824266135693... val accuracy:0.9597848355770111.\n",
      "Val loss decreased...\n",
      "Epoch: 110/350... Loss: 0.10206128470599651... Val Loss: 0.07364585995674133... val accuracy:0.9679815471172333.\n",
      "Epoch: 111/350... Loss: 0.09650370944291353... Val Loss: 0.07136986032128334... val accuracy:0.9605532884597778.\n",
      "Epoch: 112/350... Loss: 0.09566461201757193... Val Loss: 0.08615571353584528... val accuracy:0.9609375.\n",
      "Epoch: 113/350... Loss: 0.1103257571036617... Val Loss: 0.09065394848585129... val accuracy:0.9445440471172333.\n",
      "Epoch: 114/350... Loss: 0.095051861833781... Val Loss: 0.07928785309195518... val accuracy:0.9597848355770111.\n",
      "Epoch: 115/350... Loss: 0.09678859294702609... Val Loss: 0.07682133838534355... val accuracy:0.9683657884597778.\n",
      "Epoch: 116/350... Loss: 0.0934515002494057... Val Loss: 0.08110835775732994... val accuracy:0.9597848355770111.\n",
      "Epoch: 117/350... Loss: 0.1190487730006377... Val Loss: 0.08461614325642586... val accuracy:0.953125.\n",
      "Epoch: 118/350... Loss: 0.12205568949381511... Val Loss: 0.09844865649938583... val accuracy:0.9359631240367889.\n",
      "Epoch: 119/350... Loss: 0.10453569920112689... Val Loss: 0.07445178925991058... val accuracy:0.9519723355770111.\n",
      "Epoch: 120/350... Loss: 0.08742850677420695... Val Loss: 0.0669175423681736... val accuracy:0.9597848355770111.\n",
      "Epoch: 121/350... Loss: 0.09559459735949834... Val Loss: 0.08199793472886086... val accuracy:0.9679815471172333.\n",
      "Epoch: 122/350... Loss: 0.09089873995011051... Val Loss: 0.07815449312329292... val accuracy:0.9761782884597778.\n",
      "Epoch: 123/350... Loss: 0.10456389685471852... Val Loss: 0.06894107162952423... val accuracy:0.953125.\n",
      "Epoch: 124/350... Loss: 0.10587384396543105... Val Loss: 0.07675785571336746... val accuracy:0.9601690471172333.\n",
      "Epoch: 125/350... Loss: 0.10811441220963995... Val Loss: 0.1209150068461895... val accuracy:0.9363473355770111.\n",
      "Epoch: 126/350... Loss: 0.10326874706273277... Val Loss: 0.09649467840790749... val accuracy:0.9523565471172333.\n",
      "Epoch: 127/350... Loss: 0.09571545881529649... Val Loss: 0.0702999047935009... val accuracy:0.9594006240367889.\n",
      "Epoch: 128/350... Loss: 0.09173411441346009... Val Loss: 0.07711030170321465... val accuracy:0.9605532884597778.\n",
      "Epoch: 129/350... Loss: 0.09970424976199865... Val Loss: 0.11755866184830666... val accuracy:0.9441598355770111.\n",
      "Epoch: 130/350... Loss: 0.09241300510863464... Val Loss: 0.07767786085605621... val accuracy:0.9597848355770111.\n",
      "Epoch: 131/350... Loss: 0.10781136869142453... Val Loss: 0.14256174117326736... val accuracy:0.9441598355770111.\n",
      "Epoch: 132/350... Loss: 0.1091544811303417... Val Loss: 0.07628920674324036... val accuracy:0.9597848355770111.\n",
      "Epoch: 133/350... Loss: 0.09102521355574329... Val Loss: 0.08436017110943794... val accuracy:0.9594006240367889.\n",
      "Epoch: 134/350... Loss: 0.0928094582632184... Val Loss: 0.06765593960881233... val accuracy:0.9601690471172333.\n",
      "Epoch: 135/350... Loss: 0.08723418779360752... Val Loss: 0.07503617368638515... val accuracy:0.9601690471172333.\n",
      "Epoch: 136/350... Loss: 0.11281776428222656... Val Loss: 0.08277765661478043... val accuracy:0.9683657884597778.\n",
      "Epoch: 137/350... Loss: 0.12549209501594305... Val Loss: 0.0966448150575161... val accuracy:0.9594006240367889.\n",
      "Epoch: 138/350... Loss: 0.09983987764765818... Val Loss: 0.08274199441075325... val accuracy:0.9601690471172333.\n",
      "Epoch: 139/350... Loss: 0.10868153131256501... Val Loss: 0.0652118744328618... val accuracy:0.9590163826942444.\n",
      "Val loss decreased...\n",
      "Epoch: 140/350... Loss: 0.09997975826263428... Val Loss: 0.07430212944746017... val accuracy:0.9597848355770111.\n",
      "Epoch: 141/350... Loss: 0.09096432818720739... Val Loss: 0.08167689107358456... val accuracy:0.9523565471172333.\n",
      "Epoch: 142/350... Loss: 0.07539192742357652... Val Loss: 0.0742455255240202... val accuracy:0.9590163826942444.\n",
      "Epoch: 143/350... Loss: 0.09037857906272014... Val Loss: 0.07410423085093498... val accuracy:0.9597848355770111.\n",
      "Epoch: 144/350... Loss: 0.09647987689822912... Val Loss: 0.07050904631614685... val accuracy:0.9594006240367889.\n",
      "Epoch: 145/350... Loss: 0.0931396372616291... Val Loss: 0.06848885305225849... val accuracy:0.9675973355770111.\n",
      "Epoch: 146/350... Loss: 0.08884887521465619... Val Loss: 0.09265612065792084... val accuracy:0.9437756240367889.\n",
      "Epoch: 147/350... Loss: 0.0821056788166364... Val Loss: 0.06799734756350517... val accuracy:0.9672131240367889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 148/350... Loss: 0.0775083377957344... Val Loss: 0.07463276013731956... val accuracy:0.9683657884597778.\n",
      "Epoch: 149/350... Loss: 0.08424137718975544... Val Loss: 0.06275732442736626... val accuracy:0.9597848355770111.\n",
      "Val loss decreased...\n",
      "Epoch: 150/350... Loss: 0.08199444816758235... Val Loss: 0.062139518558979034... val accuracy:0.9601690471172333.\n",
      "Val loss decreased...\n",
      "Epoch: 151/350... Loss: 0.1134906377022465... Val Loss: 0.07873156853020191... val accuracy:0.9527407884597778.\n",
      "Epoch: 152/350... Loss: 0.11665785654137532... Val Loss: 0.10182767547667027... val accuracy:0.9430071711540222.\n",
      "Epoch: 153/350... Loss: 0.10874692412714164... Val Loss: 0.08447418361902237... val accuracy:0.9597848355770111.\n",
      "Epoch: 154/350... Loss: 0.08264835209896167... Val Loss: 0.0825180821120739... val accuracy:0.9601690471172333.\n",
      "Epoch: 155/350... Loss: 0.09292972243080537... Val Loss: 0.1441191304475069... val accuracy:0.9371157884597778.\n",
      "Epoch: 156/350... Loss: 0.09152570987741153... Val Loss: 0.125112596899271... val accuracy:0.9355788826942444.\n",
      "Epoch: 157/350... Loss: 0.10247603757306933... Val Loss: 0.06818871386349201... val accuracy:0.9609375.\n",
      "Epoch: 158/350... Loss: 0.0858162953518331... Val Loss: 0.08613332360982895... val accuracy:0.9597848355770111.\n",
      "Epoch: 159/350... Loss: 0.1037371326237917... Val Loss: 0.0840778611600399... val accuracy:0.9605532884597778.\n",
      "Epoch: 160/350... Loss: 0.10999756089101236... Val Loss: 0.08503170497715473... val accuracy:0.9605532884597778.\n",
      "Epoch: 161/350... Loss: 0.10591608906785648... Val Loss: 0.1299319677054882... val accuracy:0.9363473355770111.\n",
      "Epoch: 162/350... Loss: 0.09519422209511201... Val Loss: 0.07017485424876213... val accuracy:0.9601690471172333.\n",
      "Epoch: 163/350... Loss: 0.09407506200174491... Val Loss: 0.07779861986637115... val accuracy:0.9601690471172333.\n",
      "Epoch: 164/350... Loss: 0.0946653784873585... Val Loss: 0.07420456036925316... val accuracy:0.9601690471172333.\n",
      "Epoch: 165/350... Loss: 0.09035113391776879... Val Loss: 0.06776083447039127... val accuracy:0.9601690471172333.\n",
      "Epoch: 166/350... Loss: 0.08293700264766812... Val Loss: 0.10713657736778259... val accuracy:0.9515881240367889.\n",
      "Epoch: 167/350... Loss: 0.07734640905012687... Val Loss: 0.07915694639086723... val accuracy:0.9597848355770111.\n",
      "Epoch: 168/350... Loss: 0.0782459364272654... Val Loss: 0.10156434029340744... val accuracy:0.9515881240367889.\n",
      "Epoch: 169/350... Loss: 0.08911031826088826... Val Loss: 0.08588433638215065... val accuracy:0.9519723355770111.\n",
      "Epoch: 170/350... Loss: 0.09864597301930189... Val Loss: 0.09828837215900421... val accuracy:0.9519723355770111.\n",
      "Epoch: 171/350... Loss: 0.09889108190933864... Val Loss: 0.0633214469999075... val accuracy:0.9601690471172333.\n",
      "Epoch: 172/350... Loss: 0.0804738774895668... Val Loss: 0.06296265125274658... val accuracy:0.9594006240367889.\n",
      "Epoch: 173/350... Loss: 0.08557991900791724... Val Loss: 0.08240433409810066... val accuracy:0.9672131240367889.\n",
      "Epoch: 174/350... Loss: 0.0939508235702912... Val Loss: 0.06672058813273907... val accuracy:0.9597848355770111.\n",
      "Epoch: 175/350... Loss: 0.08041269425302744... Val Loss: 0.05648564547300339... val accuracy:0.9679815471172333.\n",
      "Val loss decreased...\n",
      "Epoch: 176/350... Loss: 0.08298702770844102... Val Loss: 0.0790286548435688... val accuracy:0.9605532884597778.\n",
      "Epoch: 177/350... Loss: 0.08688358248521884... Val Loss: 0.07989012077450752... val accuracy:0.9597848355770111.\n",
      "Epoch: 178/350... Loss: 0.08576855482533574... Val Loss: 0.07827882561832666... val accuracy:0.9449282884597778.\n",
      "Epoch: 179/350... Loss: 0.07470199217398961... Val Loss: 0.06575839780271053... val accuracy:0.9675973355770111.\n",
      "Epoch: 180/350... Loss: 0.09151758657147487... Val Loss: 0.06876439042389393... val accuracy:0.9672131240367889.\n",
      "Epoch: 181/350... Loss: 0.08031374875766535... Val Loss: 0.1809130571782589... val accuracy:0.9344262182712555.\n",
      "Epoch: 182/350... Loss: 0.10477446330090363... Val Loss: 0.08066483959555626... val accuracy:0.9523565471172333.\n",
      "Epoch: 183/350... Loss: 0.10473849282910426... Val Loss: 0.0774836540222168... val accuracy:0.9597848355770111.\n",
      "Epoch: 184/350... Loss: 0.07529285550117493... Val Loss: 0.062210206873714924... val accuracy:0.9527407884597778.\n",
      "Epoch: 185/350... Loss: 0.0799305943461756... Val Loss: 0.07884434796869755... val accuracy:0.9605532884597778.\n",
      "Epoch: 186/350... Loss: 0.08211875846609473... Val Loss: 0.09488299116492271... val accuracy:0.9519723355770111.\n",
      "Epoch: 187/350... Loss: 0.07751506520435214... Val Loss: 0.11142218112945557... val accuracy:0.9519723355770111.\n",
      "Epoch: 188/350... Loss: 0.07452222766975562... Val Loss: 0.07716033980250359... val accuracy:0.9601690471172333.\n",
      "Epoch: 189/350... Loss: 0.08261115103960037... Val Loss: 0.1042087934911251... val accuracy:0.9367315471172333.\n",
      "Epoch: 190/350... Loss: 0.1066571253662308... Val Loss: 0.07316051796078682... val accuracy:0.9523565471172333.\n",
      "Epoch: 191/350... Loss: 0.09417658609648545... Val Loss: 0.0823194570839405... val accuracy:0.9601690471172333.\n",
      "Epoch: 192/350... Loss: 0.1074538785032928... Val Loss: 0.08504124730825424... val accuracy:0.9679815471172333.\n",
      "Epoch: 193/350... Loss: 0.09453853747497003... Val Loss: 0.08349371701478958... val accuracy:0.9519723355770111.\n",
      "Epoch: 194/350... Loss: 0.11545405164361... Val Loss: 0.08362110331654549... val accuracy:0.9601690471172333.\n",
      "Epoch: 195/350... Loss: 0.0804516589269042... Val Loss: 0.0760053601115942... val accuracy:0.9605532884597778.\n",
      "Epoch: 196/350... Loss: 0.08525289253642161... Val Loss: 0.06853388994932175... val accuracy:0.9523565471172333.\n",
      "Epoch: 197/350... Loss: 0.07662833202630281... Val Loss: 0.0619353074580431... val accuracy:0.9515881240367889.\n",
      "Epoch: 198/350... Loss: 0.0753996972925961... Val Loss: 0.08315206877887249... val accuracy:0.9594006240367889.\n",
      "Epoch: 199/350... Loss: 0.0846050192291538... Val Loss: 0.07222011312842369... val accuracy:0.9601690471172333.\n",
      "Epoch: 200/350... Loss: 0.07665734163795908... Val Loss: 0.06325474102050066... val accuracy:0.9523565471172333.\n",
      "Epoch: 201/350... Loss: 0.07867742997283737... Val Loss: 0.06344306096434593... val accuracy:0.9761782884597778.\n",
      "Epoch: 202/350... Loss: 0.06588785179580252... Val Loss: 0.06793064926750958... val accuracy:0.9609375.\n",
      "Epoch: 203/350... Loss: 0.07752118601153295... Val Loss: 0.078043008223176... val accuracy:0.9515881240367889.\n",
      "Epoch: 204/350... Loss: 0.08583886362612247... Val Loss: 0.10580286383628845... val accuracy:0.9441598355770111.\n",
      "Epoch: 205/350... Loss: 0.0780006367713213... Val Loss: 0.0923396646976471... val accuracy:0.9437756240367889.\n",
      "Epoch: 206/350... Loss: 0.07134191070993741... Val Loss: 0.09143585152924061... val accuracy:0.9597848355770111.\n",
      "Epoch: 207/350... Loss: 0.07175869556764762... Val Loss: 0.08150837942957878... val accuracy:0.9515881240367889.\n",
      "Epoch: 208/350... Loss: 0.08114080751935641... Val Loss: 0.0710916742682457... val accuracy:0.9601690471172333.\n",
      "Epoch: 209/350... Loss: 0.10494628564144175... Val Loss: 0.08092963695526123... val accuracy:0.9597848355770111.\n",
      "Epoch: 210/350... Loss: 0.10052008616427581... Val Loss: 0.06360079906880856... val accuracy:0.9675973355770111.\n",
      "Epoch: 211/350... Loss: 0.08272605404878657... Val Loss: 0.07353376597166061... val accuracy:0.9519723355770111.\n",
      "Epoch: 212/350... Loss: 0.07247885596007109... Val Loss: 0.07487404625862837... val accuracy:0.9605532884597778.\n",
      "Epoch: 213/350... Loss: 0.0747075368805478... Val Loss: 0.11472846567630768... val accuracy:0.9523565471172333.\n",
      "Epoch: 214/350... Loss: 0.08681142857919137... Val Loss: 0.06732056755572557... val accuracy:0.9675973355770111.\n",
      "Epoch: 215/350... Loss: 0.08428233908489347... Val Loss: 0.06781514547765255... val accuracy:0.9601690471172333.\n",
      "Epoch: 216/350... Loss: 0.09253424716492493... Val Loss: 0.06406472250819206... val accuracy:0.9679815471172333.\n",
      "Epoch: 217/350... Loss: 0.08051303091148536... Val Loss: 0.06642812304198742... val accuracy:0.9683657884597778.\n",
      "Epoch: 218/350... Loss: 0.06625107153862093... Val Loss: 0.06882976181805134... val accuracy:0.9594006240367889.\n",
      "Epoch: 219/350... Loss: 0.08705556237449248... Val Loss: 0.10532910749316216... val accuracy:0.9515881240367889.\n",
      "Epoch: 220/350... Loss: 0.0931449414541324... Val Loss: 0.13899625837802887... val accuracy:0.9441598355770111.\n",
      "Epoch: 221/350... Loss: 0.14807268604636192... Val Loss: 0.14267726987600327... val accuracy:0.9523565471172333.\n",
      "Epoch: 222/350... Loss: 0.10804101265966892... Val Loss: 0.10081382840871811... val accuracy:0.9605532884597778.\n",
      "Epoch: 223/350... Loss: 0.07904228940606117... Val Loss: 0.10606447979807854... val accuracy:0.9437756240367889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224/350... Loss: 0.07249127374961972... Val Loss: 0.06360404007136822... val accuracy:0.9605532884597778.\n",
      "Epoch: 225/350... Loss: 0.07658438694973786... Val Loss: 0.05946928169578314... val accuracy:0.9590163826942444.\n",
      "Epoch: 226/350... Loss: 0.07521409541368484... Val Loss: 0.06226915679872036... val accuracy:0.9601690471172333.\n",
      "Epoch: 227/350... Loss: 0.07610742344210546... Val Loss: 0.0852387398481369... val accuracy:0.9601690471172333.\n",
      "Epoch: 228/350... Loss: 0.06785184517502785... Val Loss: 0.0687749907374382... val accuracy:0.9679815471172333.\n",
      "Epoch: 229/350... Loss: 0.07385337474988773... Val Loss: 0.06299430970102549... val accuracy:0.9605532884597778.\n",
      "Epoch: 230/350... Loss: 0.07030588512619336... Val Loss: 0.06905307620763779... val accuracy:0.9594006240367889.\n",
      "Epoch: 231/350... Loss: 0.07131438128029306... Val Loss: 0.06539619714021683... val accuracy:0.9601690471172333.\n",
      "Epoch: 232/350... Loss: 0.07565479725599289... Val Loss: 0.06416080519556999... val accuracy:0.9597848355770111.\n",
      "Epoch: 233/350... Loss: 0.07462879580756028... Val Loss: 0.1011938787996769... val accuracy:0.9597848355770111.\n",
      "Epoch: 234/350... Loss: 0.08307352351645629... Val Loss: 0.08763426542282104... val accuracy:0.9519723355770111.\n",
      "Epoch: 235/350... Loss: 0.081918948640426... Val Loss: 0.05768609791994095... val accuracy:0.9597848355770111.\n",
      "Epoch: 236/350... Loss: 0.0704036292930444... Val Loss: 0.0840076431632042... val accuracy:0.9601690471172333.\n",
      "Epoch: 237/350... Loss: 0.06781386157187323... Val Loss: 0.06031210720539093... val accuracy:0.9597848355770111.\n",
      "Epoch: 238/350... Loss: 0.07681270906080802... Val Loss: 0.07845848053693771... val accuracy:0.9601690471172333.\n",
      "Epoch: 239/350... Loss: 0.07578371294463675... Val Loss: 0.05584435351192951... val accuracy:0.9679815471172333.\n",
      "Val loss decreased...\n",
      "Epoch: 240/350... Loss: 0.0727673601359129... Val Loss: 0.07904767245054245... val accuracy:0.96875.\n",
      "Epoch: 241/350... Loss: 0.0992928718527158... Val Loss: 0.0648949146270752... val accuracy:0.9683657884597778.\n",
      "Epoch: 242/350... Loss: 0.06629041815176606... Val Loss: 0.059780918061733246... val accuracy:0.9597848355770111.\n",
      "Epoch: 243/350... Loss: 0.0625327800710996... Val Loss: 0.06579799018800259... val accuracy:0.9679815471172333.\n",
      "Epoch: 244/350... Loss: 0.0692497364555796... Val Loss: 0.07264884561300278... val accuracy:0.9679815471172333.\n",
      "Epoch: 245/350... Loss: 0.06320078608890374... Val Loss: 0.057482050731778145... val accuracy:0.9679815471172333.\n",
      "Epoch: 246/350... Loss: 0.07128032327940066... Val Loss: 0.05940810218453407... val accuracy:0.9679815471172333.\n",
      "Epoch: 247/350... Loss: 0.07774680107831955... Val Loss: 0.06550243310630322... val accuracy:0.9597848355770111.\n",
      "Epoch: 248/350... Loss: 0.07280770953123768... Val Loss: 0.06345036951825023... val accuracy:0.96875.\n",
      "Epoch: 249/350... Loss: 0.06802863456929724... Val Loss: 0.07876376062631607... val accuracy:0.9601690471172333.\n",
      "Epoch: 250/350... Loss: 0.08343325058619182... Val Loss: 0.06559760961681604... val accuracy:0.9672131240367889.\n",
      "Epoch: 251/350... Loss: 0.06589147929723065... Val Loss: 0.06156990863382816... val accuracy:0.9597848355770111.\n",
      "Epoch: 252/350... Loss: 0.0773113255078594... Val Loss: 0.07505124434828758... val accuracy:0.9437756240367889.\n",
      "Epoch: 253/350... Loss: 0.06700513372197747... Val Loss: 0.09945609793066978... val accuracy:0.9515881240367889.\n",
      "Epoch: 254/350... Loss: 0.10018644218022625... Val Loss: 0.06527438387274742... val accuracy:0.9594006240367889.\n",
      "Epoch: 255/350... Loss: 0.08486471294114988... Val Loss: 0.07979513332247734... val accuracy:0.9675973355770111.\n",
      "Epoch: 256/350... Loss: 0.09271588052312534... Val Loss: 0.11198019236326218... val accuracy:0.9433913826942444.\n",
      "Epoch: 257/350... Loss: 0.08653009900202353... Val Loss: 0.07310466840863228... val accuracy:0.9679815471172333.\n",
      "Epoch: 258/350... Loss: 0.07593773926297824... Val Loss: 0.07069535180926323... val accuracy:0.9523565471172333.\n",
      "Epoch: 259/350... Loss: 0.07334600420047839... Val Loss: 0.06024560518562794... val accuracy:0.9683657884597778.\n",
      "Epoch: 260/350... Loss: 0.0817632384908696... Val Loss: 0.07778326980769634... val accuracy:0.9515881240367889.\n",
      "Epoch: 261/350... Loss: 0.0664274231530726... Val Loss: 0.06501344032585621... val accuracy:0.9605532884597778.\n",
      "Epoch: 262/350... Loss: 0.0691658294138809... Val Loss: 0.07126034237444401... val accuracy:0.9675973355770111.\n",
      "Epoch: 263/350... Loss: 0.0698955353970329... Val Loss: 0.06034600455313921... val accuracy:0.9672131240367889.\n",
      "Epoch: 264/350... Loss: 0.0712805325165391... Val Loss: 0.06318196840584278... val accuracy:0.9679815471172333.\n",
      "Epoch: 265/350... Loss: 0.0698441502948602... Val Loss: 0.06678503938019276... val accuracy:0.9523565471172333.\n",
      "Epoch: 266/350... Loss: 0.06411006581038237... Val Loss: 0.05942714586853981... val accuracy:0.9597848355770111.\n",
      "Epoch: 267/350... Loss: 0.07294369209557772... Val Loss: 0.056357983499765396... val accuracy:0.9679815471172333.\n",
      "Epoch: 268/350... Loss: 0.0633248072117567... Val Loss: 0.07301642373204231... val accuracy:0.9601690471172333.\n",
      "Epoch: 269/350... Loss: 0.07172567086915176... Val Loss: 0.06417470425367355... val accuracy:0.9594006240367889.\n",
      "Epoch: 270/350... Loss: 0.06771677965298295... Val Loss: 0.0532864686101675... val accuracy:0.9679815471172333.\n",
      "Val loss decreased...\n",
      "Epoch: 271/350... Loss: 0.06098363269120455... Val Loss: 0.058878676034510136... val accuracy:0.9609375.\n",
      "Epoch: 272/350... Loss: 0.0847612723397712... Val Loss: 0.09577786922454834... val accuracy:0.9683657884597778.\n",
      "Epoch: 273/350... Loss: 0.08049693113813798... Val Loss: 0.05631905235350132... val accuracy:0.9679815471172333.\n",
      "Epoch: 274/350... Loss: 0.06374145749335487... Val Loss: 0.06314107961952686... val accuracy:0.9765625.\n",
      "Epoch: 275/350... Loss: 0.058821076061576605... Val Loss: 0.05862066149711609... val accuracy:0.9761782884597778.\n",
      "Epoch: 276/350... Loss: 0.06874985853210092... Val Loss: 0.0794540885835886... val accuracy:0.9594006240367889.\n",
      "Epoch: 277/350... Loss: 0.059560433185348906... Val Loss: 0.06560974754393101... val accuracy:0.9679815471172333.\n",
      "Epoch: 278/350... Loss: 0.07131839077919722... Val Loss: 0.06402375549077988... val accuracy:0.9675973355770111.\n",
      "Epoch: 279/350... Loss: 0.07730915102486809... Val Loss: 0.09407278848811984... val accuracy:0.96875.\n",
      "Epoch: 280/350... Loss: 0.08525092573836446... Val Loss: 0.05795297585427761... val accuracy:0.9601690471172333.\n",
      "Epoch: 281/350... Loss: 0.06668498894820611... Val Loss: 0.06906497851014137... val accuracy:0.9679815471172333.\n",
      "Epoch: 282/350... Loss: 0.058883353912582... Val Loss: 0.07952754758298397... val accuracy:0.9675973355770111.\n",
      "Epoch: 283/350... Loss: 0.07008678962786992... Val Loss: 0.07746562175452709... val accuracy:0.9683657884597778.\n",
      "Epoch: 284/350... Loss: 0.07313578420629104... Val Loss: 0.05342355044558644... val accuracy:0.9765625.\n",
      "Epoch: 285/350... Loss: 0.0639770928149422... Val Loss: 0.062050918117165565... val accuracy:0.9675973355770111.\n",
      "Epoch: 286/350... Loss: 0.06588834462066491... Val Loss: 0.06692635826766491... val accuracy:0.9679815471172333.\n",
      "Epoch: 287/350... Loss: 0.08832936749483149... Val Loss: 0.06819187942892313... val accuracy:0.9765625.\n",
      "Epoch: 288/350... Loss: 0.07153625662128131... Val Loss: 0.05664696730673313... val accuracy:0.9512038826942444.\n",
      "Epoch: 289/350... Loss: 0.05872246716171503... Val Loss: 0.057420939207077026... val accuracy:0.9594006240367889.\n",
      "Epoch: 290/350... Loss: 0.05730689441164335... Val Loss: 0.08049078285694122... val accuracy:0.9605532884597778.\n",
      "Epoch: 291/350... Loss: 0.06517357937991619... Val Loss: 0.08042997121810913... val accuracy:0.9679815471172333.\n",
      "Epoch: 292/350... Loss: 0.06695195644473036... Val Loss: 0.06287321913987398... val accuracy:0.96875.\n",
      "Epoch: 293/350... Loss: 0.06113449251279235... Val Loss: 0.06701734475791454... val accuracy:0.9512038826942444.\n",
      "Epoch: 294/350... Loss: 0.06009326595813036... Val Loss: 0.05862210784107447... val accuracy:0.9605532884597778.\n",
      "Epoch: 295/350... Loss: 0.07004636712372303... Val Loss: 0.06418558955192566... val accuracy:0.9605532884597778.\n",
      "Epoch: 296/350... Loss: 0.08237284095957875... Val Loss: 0.09903960302472115... val accuracy:0.9683657884597778.\n",
      "Epoch: 297/350... Loss: 0.10578983121862014... Val Loss: 0.07139386795461178... val accuracy:0.9597848355770111.\n",
      "Epoch: 298/350... Loss: 0.06003223344062766... Val Loss: 0.12025584559887648... val accuracy:0.9594006240367889.\n",
      "Epoch: 299/350... Loss: 0.06322743988130242... Val Loss: 0.0537598580121994... val accuracy:0.9594006240367889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300/350... Loss: 0.06513889878988266... Val Loss: 0.07682128250598907... val accuracy:0.9683657884597778.\n",
      "Epoch: 301/350... Loss: 0.07215813376630346... Val Loss: 0.07610444352030754... val accuracy:0.9679815471172333.\n",
      "Epoch: 302/350... Loss: 0.08437581701825063... Val Loss: 0.06372133828699589... val accuracy:0.9679815471172333.\n",
      "Epoch: 303/350... Loss: 0.08609186656152208... Val Loss: 0.0637755086645484... val accuracy:0.9512038826942444.\n",
      "Epoch: 304/350... Loss: 0.06622495564321677... Val Loss: 0.06731412373483181... val accuracy:0.9605532884597778.\n",
      "Epoch: 305/350... Loss: 0.06964895656953256... Val Loss: 0.06058814004063606... val accuracy:0.9597848355770111.\n",
      "Epoch: 306/350... Loss: 0.08379439101554453... Val Loss: 0.07824360951781273... val accuracy:0.9679815471172333.\n",
      "Epoch: 307/350... Loss: 0.08001306963463624... Val Loss: 0.05576753430068493... val accuracy:0.9605532884597778.\n",
      "Epoch: 308/350... Loss: 0.06406342362364133... Val Loss: 0.054242344573140144... val accuracy:0.9679815471172333.\n",
      "Epoch: 309/350... Loss: 0.07280109527831276... Val Loss: 0.10265389829874039... val accuracy:0.9433913826942444.\n",
      "Epoch: 310/350... Loss: 0.06977785332128406... Val Loss: 0.06887931004166603... val accuracy:0.9679815471172333.\n",
      "Epoch: 311/350... Loss: 0.062436734015742935... Val Loss: 0.0611724192276597... val accuracy:0.9605532884597778.\n",
      "Epoch: 312/350... Loss: 0.06818297132849693... Val Loss: 0.05676339380443096... val accuracy:0.9761782884597778.\n",
      "Epoch: 313/350... Loss: 0.0772809103752176... Val Loss: 0.061258263885974884... val accuracy:0.9761782884597778.\n",
      "Epoch: 314/350... Loss: 0.0620961906388402... Val Loss: 0.08550586178898811... val accuracy:0.9609375.\n",
      "Epoch: 315/350... Loss: 0.07537619086603324... Val Loss: 0.05755891837179661... val accuracy:0.9679815471172333.\n",
      "Epoch: 316/350... Loss: 0.06101378689830502... Val Loss: 0.060185592621564865... val accuracy:0.9675973355770111.\n",
      "Epoch: 317/350... Loss: 0.07004767407973607... Val Loss: 0.060515254735946655... val accuracy:0.9761782884597778.\n",
      "Epoch: 318/350... Loss: 0.06002607103437185... Val Loss: 0.0529450960457325... val accuracy:0.9679815471172333.\n",
      "Val loss decreased...\n",
      "Epoch: 319/350... Loss: 0.06025870827337106... Val Loss: 0.05539755895733833... val accuracy:0.9836065471172333.\n",
      "Epoch: 320/350... Loss: 0.06484998855739832... Val Loss: 0.0980900451540947... val accuracy:0.9609375.\n",
      "Epoch: 321/350... Loss: 0.06322468879322211... Val Loss: 0.08470089547336102... val accuracy:0.9683657884597778.\n",
      "Epoch: 322/350... Loss: 0.07199311318496864... Val Loss: 0.07082591950893402... val accuracy:0.9679815471172333.\n",
      "Epoch: 323/350... Loss: 0.06476323337604602... Val Loss: 0.057990361005067825... val accuracy:0.9601690471172333.\n",
      "Epoch: 324/350... Loss: 0.05617485924934348... Val Loss: 0.06079314276576042... val accuracy:0.9515881240367889.\n",
      "Epoch: 325/350... Loss: 0.06050694858034452... Val Loss: 0.057001277804374695... val accuracy:0.9601690471172333.\n",
      "Epoch: 326/350... Loss: 0.05977216362953186... Val Loss: 0.06853403337299824... val accuracy:0.9757940471172333.\n",
      "Epoch: 327/350... Loss: 0.05106450151652098... Val Loss: 0.06457871943712234... val accuracy:0.9601690471172333.\n",
      "Epoch: 328/350... Loss: 0.05569305635678271... Val Loss: 0.060603477992117405... val accuracy:0.9523565471172333.\n",
      "Epoch: 329/350... Loss: 0.06604201067239046... Val Loss: 0.06450178846716881... val accuracy:0.9679815471172333.\n",
      "Epoch: 330/350... Loss: 0.07269329267243545... Val Loss: 0.12241187691688538... val accuracy:0.9597848355770111.\n",
      "Epoch: 331/350... Loss: 0.06978768296539783... Val Loss: 0.0699461717158556... val accuracy:0.9675973355770111.\n",
      "Epoch: 332/350... Loss: 0.06944263726472855... Val Loss: 0.06621650233864784... val accuracy:0.9679815471172333.\n",
      "Epoch: 333/350... Loss: 0.07208736178775628... Val Loss: 0.05807493254542351... val accuracy:0.9601690471172333.\n",
      "Epoch: 334/350... Loss: 0.07274900958873332... Val Loss: 0.061643607914447784... val accuracy:0.9597848355770111.\n",
      "Epoch: 335/350... Loss: 0.0650218593267103... Val Loss: 0.06166289374232292... val accuracy:0.9761782884597778.\n",
      "Epoch: 336/350... Loss: 0.06654838255296151... Val Loss: 0.05343597289174795... val accuracy:0.9754098355770111.\n",
      "Epoch: 337/350... Loss: 0.05551976850256324... Val Loss: 0.05709793046116829... val accuracy:0.9597848355770111.\n",
      "Epoch: 338/350... Loss: 0.06004314155628284... Val Loss: 0.0652450229972601... val accuracy:0.9683657884597778.\n",
      "Epoch: 339/350... Loss: 0.05827457721655568... Val Loss: 0.057561490684747696... val accuracy:0.9519723355770111.\n",
      "Epoch: 340/350... Loss: 0.05768401199020445... Val Loss: 0.07013144344091415... val accuracy:0.9605532884597778.\n",
      "Epoch: 341/350... Loss: 0.05713948813111832... Val Loss: 0.0597369484603405... val accuracy:0.9597848355770111.\n",
      "Epoch: 342/350... Loss: 0.05634208209812641... Val Loss: 0.06527009606361389... val accuracy:0.9675973355770111.\n",
      "Epoch: 343/350... Loss: 0.046507118580242... Val Loss: 0.05726605746895075... val accuracy:0.9761782884597778.\n",
      "Epoch: 344/350... Loss: 0.06942996600021918... Val Loss: 0.06163901276886463... val accuracy:0.9683657884597778.\n",
      "Epoch: 345/350... Loss: 0.07672222098335624... Val Loss: 0.1174371987581253... val accuracy:0.9355788826942444.\n",
      "Epoch: 346/350... Loss: 0.07007082117100556... Val Loss: 0.07367991097271442... val accuracy:0.9675973355770111.\n",
      "Epoch: 347/350... Loss: 0.051701889373362064... Val Loss: 0.0729354340583086... val accuracy:0.9765625.\n",
      "Epoch: 348/350... Loss: 0.05815219630797704... Val Loss: 0.08620288781821728... val accuracy:0.9765625.\n",
      "Epoch: 349/350... Loss: 0.05765595659613609... Val Loss: 0.08142675086855888... val accuracy:0.9683657884597778.\n",
      "Epoch: 350/350... Loss: 0.06337573969115813... Val Loss: 0.06348920799791813... val accuracy:0.9757940471172333.\n",
      "min loss 0.0529450960457325\n",
      "Training time is: 19.276039361953735 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEUCAYAAAAr20GQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd809X+x/HXaZouaBllUxAQmUJVQFGZFUGUDSpeEBAQBb0q6lXR+xO9bq/XgRtRZCkIshGQPUWgCMiGllJKB110jzQ5vz++abpSymhsMZ/n48GDJD355iTQ7ztnfpXWGiGEEMKjoisghBCicpBAEEIIAUggCCGEsJNAEEIIAUggCCGEsJNAEEIIAUggCHHJlFJVlVI+Ljiud3kfU4grIYEg3I5Sar5Squsllm2ilBpsvzsKmH4ZrxOklGp+CUV3KKXutT9nh1Lqpkt9DSHKkwSCcEc5gAVAKfWGUuqwUmq7/U+cUqplobL3A93tt/OA86UdVCnlrZRShR4aA7xc6OeqeBn7azUBNtkfsgAZ9p/5XeH7E+KKeFZ0BYT4qyilagADgKbAvUqpdIyT/PvABnuxDykIC2/gccCklLoHqAZ4KaX62cv6Az201ift99cDVZRSNvtrJNiPkwBEYPy+mTECJsH+nMeARVrrrEJV1UopD4yWwzSt9czy+xSEKJ0EgnAnViARo4WQAmQBW4AuGN/mAQ4Cqfbb/wJ2aa1HACil/gnU01q/4uzgWuuuSqkewCHgY2AGxu/YSGAy0ERr/Ud+eaVUY2Ai8D8nh3vMXs95V/ZWhbh8EgjCbWitU4GVSqlhwA4gBuOEW/z34ALwGfAjkFno8RrYu3OcUUrVsz9nmJMftwcWKKX6aa33KqXMwHwg1EnZO4FXgDu11rmX8t6EKA8SCMKtKKVaA82BWcA/gepa61ZKqWDgNHAv0NpefAuQae8CAqgLWJVSo+z3PYAArXU9+/1FwGta6x1KqSftPzcBHlrrLUqpR4Hv7YPG3sBW4DhwfbFqfgAM1lqfKdc3L0QZJBCE27CfiOfY747XWm8rNL77GfCs/bYG0FoHFXquN3AMiANCtNaFWw75hgNvKaWGAJ2AlhihEKSUWgN8CXTUWucB6cBLSqkx9uObgOcxWgf9tNbby+VNC3EZZJaRcCcHgGCMbhqL/TEv+9/VMcYPCj9W2L+A1cAXwAz7CbwIrXUUEAD0xuheygWygXCgDbBOa51dSt02YLRMDgBhl/WuhCgnEgjCbWiDrdjDjyil7sCY9dMd+INiaw3sXT0jgZe11rMxxhjWKqVaOXmNwYAvcAL4t9a6M7ALeLqUVkW+h7TWYzBaDoVfu4+z8BHCFSQQhFtRSt2GMYaQa+8GCsIYT3gaaAAsA+5TSnkopW5TSi0BJgF9tNYX7Id5AqO1sFcptar4CdveCvgHMEcp9SuQobVeUkqVPAGltY7JfzrQ0F7XRsAKQNYjiL+EjCEId9MHWIsxNXQ+xrf9u7TWkcBBpdQK4A2MNQftgd8xvr07unq0cZnB/ymlZgIttNZWAKXUBIxQaQR0BI4CUcBQpVR7jK6gdGCq1jq/y8qPol1UPwNf2EPGBryrtU4r/49BiJKUXEJTiPKhlBoJBGIEwe78FoV9kVkb4GaMGUezKq6WQpROAkEIIQQgYwhCCCHsJBCEEEIA19igcq1atXSTJk0quhpCCHHNCA0NTdBa176UstdUIDRp0oS9e/dWdDWEEOKaoZS65C1QpMtICCEEIIEghBDCTgJBCCEEcI2NIQgh3IvFYiEqKors7NL2BBT5fHx8CAoKwmw2X/ExJBCEEJVWVFQU/v7+NGnShKKXqxaFaa1JTEwkKiqKpk2bXvFxpMtICFFpZWdnExgYKGFQBqUUgYGBV92Sco9AsFggNbXsckKISqcyhEFWVhZWqxUwvo1fqry8PFdVqYTy+Jz+9oFgyc7k1QH+LH9/XEVXRQhxjfr000+ZNcvYk7Bnz57ExcVx8uRJoqOjGT58OAAnT57kueeeczwnMjKS+++/v8Sx4uLiGDFiBADvvfcet9xyCz169OCmm27i66+/LlI2NDSUrKws1q9fz4cffuiqt+fwtw8ET29fvr3JxoLELRVdFSHENWrixImsWLECALPZTK1atZg0aRJmsxlvb28Avv76a+69917CwsIYPXo048aNIyEhgccff5wxY8aQkJBAeHg4S5cuJSoqih07duDp6cmbb77J/PnzefXVV4sMCGutGT16NFlZWWRlZZGZmel4PL+1Ut7+9oPKSilu97iO33zCjK6jqxiBF0K4n7S0NLKzs5k7dy5gBILJZMLb2xulFEopwsPDOXr0KEOGDGHcuHGsW7eOoUOHsnz5cnx9fR3HSk9Px8fHB5PJhMlkolOnTvz666/s3LkTgAEDBjjKzpkzB4vFwpAhQ0hMTCQjI4P169djs9kYNWoU48ePL/f3+rcPBIDbG93Bz3GniNu1nrpd+1Z0dYQQ15CtW7fy5Zdf0q5dO9555x2nffWRkZFkZWUxaNAgNm/eTO/evYmIiKBNmzZ888039OrVCzC236latSqbNm0iODiYiRMnEhUV5TiO1Wrl1ltvBSAiIoKdO3cSGBjIvHnzOH/+PJMnT3bpe3WPQOg0GFbO5rc9SxgkgSDEtemZZ2D//vI95k03wccfX7TIfffdR926dR1dRs706NEDb29vfvrpJ9q0aUPr1q356KOPWLlyJZ6enuTm5uLl5UVcXBwLFixg48aN/PDDDyQkJLB+/XpWrFhB3759GTZsGADR0dGsWLGCjRs3Asb0W601y5YtA4xB7pkzZ9KmTZty+iAMbhEIbVp1hZUQnhJR0VURQlyj8lsGzmYZZWVlMXToUO6//34+++wzjhw5wtSpUzl58iRr1qyhTp06LF68mGPHjpGXl0dISAjjxo1j8eLFAEybNo2+ffs6jt2gQQP27NnjOH63bt2oUaMGCxcuxMvLq8Trlxe3CIQqPgEAZGbJ1FMhrlllfJN3tR07dvDxxx9js9mAosHg6+vLokWLaNSoEUopBg4cyLfffkunTp24/vrrHc/p3r07bdu25fnnnwegbdu2HDt2jJtuugmbzUaHDh1KvO7HH39Mt27d6NmzJ6NHj+b77793DGSXN7cIBLPJjKdNAkEIcWUiIiJITEwkICCATp06AQVrE/Ly8ggLC+Ptt98mMDCQFi1asG3bNt544w1WrlxJvXr1ePfdd2nevDmdOnUiMTERDw9jguekSZMYP348r732Gn379uWZZ55xvGZWVhZvvvkmhw8fZtGiRXh6ehIbG0vHjh0ZP348Y8aMoVq1auX6Pv/2007z+VlNZOZmVHQ1hBDXoM6dO/PDDz8wc+ZMxo4dC8DGjRuxWq1YLBaaNm3KkiVLmDVrFj169OCJJ54gLS2NRYsW0apVK2bMmEG9evXIzc1lypQpjBgxgunTpzN58mSmTZtGly5dWL58OWvXriUyMtIxk8jf35/Fixfj6Wl8dx8xYgTLli0jOTmZqlWrlvv7VJez6q6idezYUV/pBXLqv+JD/+iqTJ+ZUM61EkK4ytGjR2ndunVFV8NBa10pVk6XxtnnpZQK1Vp3vJTnu08LQZnJypMdE4UQV64yh0F5cJ9A8PAm0yaBIIQQpXGfQDD5kqmskJtb0VURQohKyaWBoJQyK6VKXc2hlPJRSq1USh1QSs1RLmyP+Zl9yTQDiYmuegkhhLimuSwQlFK+QChw90WKjQSitNbBQI0yyl4VP6+qEghCiMtmtVpLbCb3559/8u9//7vIY0ePHiUmJgYoue211Wp1rFtwy91OtdZZWuv2QNRFioUA6+y3NwI9XVUfPx9/CQQhxGVbv349ffv2pWvXrjz11FMAfP755zRu3Jg1a9Y4ymVlZTFq1ChiY2O555576NevH7Vr16Zfv3707duX8PBw2e20DIFAiv12KtDSVS/k52sPhAsXXPUSQoi/oT59+hAeHk6HDh3YsGED8+bNo127dkyYMIGXX36ZXbt2MXbsWOrXr8+sWbOoV68e69evB6BXr16sXLnScayIiAjZ7fQiEoD8pXbV7PeLUEpNACYANG7c+IpfyNFllJNzxccQQrin7du3O7alWL58OTk5Ofzyyy9orQkJCWHmzJns27cPPz8/fvzxR5599tkiq47z8vLw9PSU3U7LsAHoDfyM0X30UfECWuvpwHQwFqZd6Qv5edsD4SqvOSqEqBjPrHmG/bHlu9vpTfVu4uN7Lr5HktVqxdvbm/j4eNq1a8eLL75ITk4O4eHhNGnShOTkZBo2bEhUVJRjXCEpKQmbzUZcXBy9evXCbDazevVq2e00n1KqKfCE1vr5Qg/PA4YopQ4CBzACwiXyA0FnZ/P3XloihChPubm5REZGMm/ePCIiIoiNjaVx48bMmjWLIUOGEB4e7tisDnD09QPUrVvX0X0EyG6nWuvm9r9PA88X+1kO0M/VdQBjUNnmAbnZ6bhmn0AhhCuV9U3eVeLi4nj22WfJyclh9uzZ3H333Xz22WeYTCY2bdrECy+8ABiDvWvWrOGjj0p0dDhU9t1O3Wdhmq8/AJnZaRVcEyHEtSQ+Pp7c3FxuueUWtNY0bNiQe+65h++++47o6GgaNmwIGFdWu/POOwkPD2fbtm2MHz+e48eP069fP+666y5WrVqFzWYrsdvpk08+ycCBA+nbty8333yz43WzsrJ45ZVX2Lx5M6+99hp33XUX/fr1o2PHjnzyySekpKQ4re/VqOgxhL+Mn6/9mgg5GdSo4LoIIa4dnTp1okaNGtx///0EBwczadIkx3bXACEhIWzevJlhw4YxaNAgqlSpUuqxsrOzmTJlCk888QTTp09n9erVTJs2jTZt2rB8+XJefPFFgoODCQoKYtSoUXTo0IHFixc7AmTEiBHcfvvtzJ49W3Y7vZrdTucdnMfIJSM5YZnIDW9+Uc41E0K4QmXZ7VRrTXx8PHXq1KnoqlyU7HZ6ifzMfgBkWuSaCEKIy6OUqvRhUB7cLxDkIjlCXFOupV6MilQen5P7BYIls4ySQojKwsfHh8TERAmFMmitSUxMxMfH56qO4z6DyvmBkJdVwTURQlyqoKAgoqKiiI+Pr+iqVHo+Pj4EBQVd1THcJhC8TMZijlyLbF0hxLXCbDbTtGnTiq6G23CbLiNHIFglEIQQwhm3CwRLnlwxTQghnHG7QMi1SiAIIYQzEghCCCEACQQhhBB2bhMIZpNxabpcm6WCayKEEJWT2wSCo4UggSCEEE65TSCYlAmlIVfnVXRVhBCiUnKbQFBK4YWJXC0tBCGEcMZtAgHACxMWm7WiqyGEEJWS2wVCLnkgG2UJIUQJ7hUIypNcDyBPxhGEEKI4twoEszKRawJyZD8jIYQozq0CwUuZjUDIzq7oqgghRKXjXoHg4SmBIIQQpXCzQDBLl5EQQpTCzQLBS1oIQghRCvcKBJO9hZArG9wJIURx7hUIHl5YJBCEEMIptwoEc34LwSLbVwghRHFuFQheJi/pMhJCiFK4VyB4eksgCCFEKdwrEKSFIIQQpXJJICilfJRSK5VSB5RSc5RSykmZKkqpZUqpHUqp911Rj+KkhSCEEKVzVQthJBCltQ4GagB3OykzAtiltb4TaKuUau2iujg4AkEGlYUQogRXBUIIsM5+eyPQ00mZHMDP3nrwAVz+td3L7CMtBCGEKIWrAiEQSLHfTgVqOinzA9AXOAoc01qHOTuQUmqCUmqvUmpvfHz8VVXKbPbG4oEEghBCOOGqQEgAqtlvV7PfL24K8JXWuhVQUyl1h7MDaa2na607aq071q5d+6oq5eUpLQQhhCiNqwJhA9DbfjsE2OSkjD+Qv6lQDlDVRXVx8PKSQBBCiNK4KhDmAQ2VUgeBJCBMKfVBsTKfAxOVUr8Bvhgh4lJeZl9sHmDNld1OhRCiOE9XHFRrnQP0K/bw88XKRAB3uuL1S+Pl5QtAbm4Wvn/lCwshxDXAvRammb0ByLVkVXBNhBCi8nGrQDCbvADIzZXrIQghRHFuFQhe+YFgkUAQQoji3DIQLHkyqCyEEMW5ZSBIC0EIIUpyy0DIkRaCEEKU4FaBYPYwA9JlJIQQzrhXIJgkEIQQojTuFQj5LQSrbH8thBDFuVcg5LcQrLKXkRBCFOdegeAYQ5BAEEKI4twrEEzSZSSEEKVxr0DwkC4jIYQojXsFQn4LwZZXwTURQojKx70CIb+FYJMuIyGEKM69AsEkgSCEEKVxr0DwkC4jIYQojXsFgowhCCFEqdwrEPJbCEggCCFEce4VCPYWQq62VnBNhBCi8nGvQMhvIWhpIQghRHFuFQiOK6ZJC0EIIUq47EBQSrVRSpldURlXM3mYUIBFabDZKro6QghRqVxyICilrldK/QQsA653XZVcy4wJiwmwyFoEIYQo7JICQSk1AVgKLABaaq2PubRWLmTGhMUDyJX9jIQQojDPSyy3AvhO62t/NNas7C0ECQQhhCjiooFgbxlkF7pf+Me5wGqtdYprquYa0kIQQgjnymoh1KBQIBTTBugIPF+uNXIxs4enjCEIIYQTFw0ErfV7AEoppbXWhX+mlLoeuM6FdXMJs/KUFoIQQjhxqbOM3lZKrVRK9cl/QGsdprXe6KJ6uYxZecoYghBCOHFJgaC1ngK8BPRRSv2hlLr1YuWVUj72ADmglJqjig0+FCr3glJqm1JqtVLK6/Krf/nMHtJCEEIIZy55HYLW+pDW+lngXuBkGcVHAlFa62CMcYi7ixdQSjUD2mqtuwKrgaBLrvVVcIwhSCAIIUQRl7oOwSf/ttY6RmudrJSqepGnhADr7Lc3Aj2dlLkLqKGU2gp0BU5fWpWvjtnDbLQQZFBZCCGKuNQWws7Cd+zdO38opdqUUj4QyJ+OmgrUdFKmNhCvte6G0Tro4uxASqkJSqm9Sqm98fHxl1jd0plNZmkhCCGEE5caCMnF7k8F/tBaHymlfAJQzX67mv1+canAcfvtcKChswNpradrrTtqrTvWrl37EqtbOkcLQQJBCCGKuNRAcGwPqpR6BmiPMU5Qmg1Ab/vtEGCTkzKhQCf77eYYoeBy0kIQQgjnSg0EpVQLpdRjSql/AHWVUq8qpRYDOVrr/lrri51R5wENlVIHgSQgTCn1QeECWuvfgASl1B7guNZ699W/nbKZTV7kSiAIIUQJF1uY5gmYgQDAC/AG6gG1lVImrUu/qIDWOgfoV+zhEiuatdYTL7vGV8ns6SWDykII4USpgWAfHzgCoJQaqrV+RSnlATwJ/KqUuk9rXdq2FpWWl6e3dBkJIYQTlzqGoAC01jat9TTgJ2C2y2rlQmaTlwwqCyGEE5cVCPm01l8DjZRSztYXVGpmaSEIIYRTl3Q9BK31XU4eflBrHVnO9XE5s1laCEII4cxlXVNZKdWq0N3k0vYoqswcLQQZVBZCiCLKDASllIdSaqz97sxCPxoD/J8rKuVKZrOPtBCEEMKJMgNBa20DRtnvZoNj64pHuQYHls2eXjKGIIQQTlxql5HN/rdWSnkC3wPvaq0jXFEpV5JZRkII4VxZ11SeDPgBjZVSTwONgYXANK21s+0oKj2zh33riiwJBCGEKKysFsJB4ACQCcQDGmPn0p72RWrXHLPJjNUDtEUCQQghCivrpL4d2AEkaK1/wLjoTXeM/YkWubpyrmD2MANgyb3mFlkLIYRLlRUIbYGtQH2lVA2MFgJa64+BI0qpF1xcv3LnZTKu1JlrkUAQQojCLhoIWut9Wut2wNsYexgVvkram0BHF9bNJSQQhBDCuUtdqTwHQCk1o9Bj2Uqp51xVMVfx9vQGICdPAkEIIQora5bRo4AFOAQMBtrYFyeHAn8AfYCnXFzHcuVoIeTJoLIQQhRWVgthEvA6MBDj6mb32B83AauB/q6rmmt4m+wtBKu0EIQQorCyBpWTgZ2F7vcF1mCMH7yqtc5yVcVcpaDLSFoIQghRWFmBUA24Jf+O1nqV1ro38B3wWrHN7q4J+S2EXGtOBddECCEql7K6jBYArYFfgSCl1HKMayMcxtjL6ENgqEtrWM7yxxByrNJCEEKIwi4aCFrr95VStwNWYCLwkNbaseOpUmqNi+tX7hxdRjbZ/loIIQora5bRXKAhRiBEAiFKqa72H9uAea6tXvlzDCrbpIUghBCFldVl9CwwCGPb61XAP+zPmQ3UAt4CrqlN7vJbCLnSQhBCiCLK6jI6r5RaB9i01olKqQVAK/vtJGDEX1LLcuQYQ5BAEEKIIspcqay1Pl3o9nngvP22Bq65qTqOLiMtgSCEEIVdk1tYXw3HoLIEghBCFOF+gZC/DiHvmmvcCCGES7lfIBSedpqXV8G1EUKIysPtAsExqGwC0tMrtjJCCFGJuF0gOAaVPYG0tIqtjBBCVCJuFwgmDxMmPMg1IYEghBCFuCQQlFI+SqmVSqkDSqk5yn4RhVLKTlZKrXdFPUrj7WE2uoxSU//KlxVCiErNVS2EkUCU1joYqAHc7ayQUuo6YIyL6lAqLw+zdBkJIUQxrgqEEGCd/fZGoGcp5T4BprioDqXyNnkbLQQJBCGEcHBVIAQCKfbbqUDN4gWUUv8ADgBHLnYgpdQEpdRepdTe+Pj4cqmct6e3jCEIIUQxrgqEBIyL62D/O8FJmX7AXcB8oINS6klnB9JaT9dad9Rad6xdu3a5VM7b08foMpIxBCGEcHBVIGwAettvh+BkR1St9T+01l2A4UCo1vozF9WlBG8vX+kyEkKIYlwVCPOAhkqpg0ASEKaU+sBFr3XZvDy9yTErCQQhhCikzN1Or4TWOgejS6iw50spGwH0ckU9SuNt8ibX21O6jIQQohC3W5gGxqByjpeHtBCEEKIQ9wwEkzc5ZgkEIYQozC0DwcvkJYEghBDFuGUgeHt6k+upICWl7MJCCOEm3DMQTN7GOoSkpIquihBCVBruGwgmJBCEEKIQ9wwET2+yTTa4cEGumiaEEHZuGQjVvKtxgWw0QHJyRVdHCCEqBbcMhFp+tcjDRpo3kJhY0dURQohKwS0DIdAvEIBEXyQQhBDCzj0DwdceCH5IIAghhJ17BoK9hZAggSCEEA7uGQi+0mUkhBDFuWcg5I8hVPWQQBBCCDu3DIQaPjVQKBJr+sjiNCGEsHPLQDB5mKjuU53E2lVg82bIzq7oKgkhRIVzy0AAo9sooUMbOHECPv+8oqsjhBAVzm0DoZZfLeIDTAx5xI/lR5dWdHWEEKLCueQSmteCJtWbsPDwQqzXWTmZupcBFV0hIYSoYG7bQni5y8uO28FRFtC6AmsjhBAVz20DoV3ddswbMg+AdA8rxMRUcI2EEKJiuW0gADx444Pc6d+WVG+MwWUhhHBjbh0IANWq1SHFGzhwoKKrIoQQFUoCoXo9Uv3NsFRmGgkh3JvbB0KAdwApVc2wZQucPl3R1RFCiArj9oFQzbsaqaY88PWFO++UK6gJIdyWBIJPNXJsueQsmm/MNNq6taKrJIQQFUICwbsaACm3BoPZDLt2lf+LLFkCX35Z/scVQohy5PaBEOAdAECqyoWbb3ZNIAwZApMmlf9xhRCiHLl9IFTzsbcQslOgc2fYvRsyMiA2FlJTr/4FMjMLbv8NxyeyLFlkWbIquhpCiHLgkkBQSvkopVYqpQ4opeYopZSTMkopNUsptUsptVwpVSH7KuV3GaXmpMLw4cYJ/PPPoX596Nbt6l9g796C28eOXbTovph9ZOddW1txj1s+jod+fqiiqyGEKAeuaiGMBKK01sFADeBuJ2XuBDy11p2BAKC3i+pyUfldRik5KXD77dCnD7zxhvHDAweMPY5CQ698r6PCXVAXCYSkrCRu/eZWfvjzB6c/T85K5v6F95OQmXBl9XCRiAsRnL4g03WF+DtwVSCEAOvstzcCPZ2UiQM+sd/OdVE9ylSkywhg4kR+rZvOyhb2AkuXQseO8P33V/YCYWFQsyZ4eV00EBIzE7Fqa6kn/C/2fMGiI4v4387/XVk9XCTDkkF6bnpFV0MIUQ5cFQiBgP0MSypQs3gBrfVJrfVupdRgwAtY6+xASqkJSqm9Sqm98fHx5V7RelXr4Wf2Y1vkNk4mnmRXuxo829eDifeBBvj5Z6Pgla5kjo83up9uuOGigZCaY4xXZORmOP15ni0PALPJfGX1cJFMS6YEghB/E67qt08AqtlvV7PfL0EpNQB4GuivtbY6K6O1ng5MB+jYsWO571HtZ/ZjRLsRfLPvG77941vjwVrGX8drQasFC4w727eDxWJMTb0c8fFQuzZUrQpnzpRaLD8QMi2ZTn+eHwieHpXrEhYZuRmk5aRVdDWEEOXAVS2EDRSMCYQAm4oXUErVA/4F3Ke1rtAzyjOdn6GBfwP6t+hf5PF1N3hAXp4RAklJ8OSTEBUFb70Fv/56aQc/fx7q1IHGjSEystRiKTlGg+qaCwRLBll5WVhtTvNcCHENcVUgzAMaKqUOAklAmFLqg2JlRgP1gbVKqe1KqbEuqkuZ2tRuw7lnz7Fs+DLHY3Wq1GFLt+uMO+PHw9NPw/Tp0KgR/PvfxuBz9iXMCMpvIVx3nTHtNM159jm6jCwX7zLyUJVrpnB+gJVWb3fV/8f+LDi0oKKrIcRlccnZRWudo7Xup7Vur7V+WGt9Wmv9fLEy72mtm2utu9j/fOeKulwOpRTv3vUu/Vv0p0vjLhyoB/TubQTCRx/BwYNGMPS2N34+++ziB7RYIDmZT2qHMdlns/HY2bNOi16sy+iNLW+w4+wOALKSz1/JW3OJXGuuI6ik26iA1ppVJ1ax8+zOiq6KEJelcn3drARe7PIiyx9aTnDdYMJSIkhf8TPccgvplgx218xCf/QRrFkDAwbAK68YYwulSTCGTuZ4HuG7tK3GIHUp3UalBUJqTiqvbn6V36J+AyDj848hOvqq32d5KFxXGVgukJ2XjUbLZyKuORIIpQiuG4xGc+j8IQDe2voWt824jTHLxoBSxE17G48puWwc2xMOH2bK+incM/eeogeJj8em4Kg1jtS8DKL9KXVgOX/aa/Gul1NJp4rcz/DUxlTWSqDwjKi0XGkh5MsPynSLBIK4tkgglKJ93fYAHIg1rqS2/vR6AOYenEtt5eWoAAAgAElEQVRKdgpb0g+jFdz3QB4NfrqNX44uY+eZ7cZ1FfKvzxwfT0R1yNQ5AByp61FqIJTWQjgVdbDI/UwzxrYaheTk5aCvdOFcbOylD5AXUzi85NtwgfzPpbQpxEJUVhIIpWhSvQlBAUHMPzyftJw0/oj5gzsa3YFN29gWuY2YNOOkn22GGI8MDiYdJS0vg5Q+PWDECOMg589zuHbBMY+0qgknTzp9vdRc54FwMnxPkfsZXhQEDsaCtprv12Rd+Dou29mzxhqJPn2M2VCXSbqMnHO0EOQzEdcYCYRSKKWY3HkymyM2E/BuAFZt5cU7X8Tb5M2m05s4mnDU6fOimgXCpk2waBGcOsWhOsbj/l7+HG5SBQ4fLvmktDRSzhpdQ8W/VZ6KOVLkfoaZIoEQmRJJpiWTI/FFy12SFSsKVTzqsp9epMtIBpUd8j+XShEINhtMmQKnTpVdVrg9CYSLmNBhAkNaD2H4jcN5oO0D3N3sbkKahvB16NesObXG6XPOzvsS6taF+++HV1/lcKuaNApoxK0Nb2VvzWyjhZBbbKeOXr1IPbAbsH+7zM11XM7zZGrRfYIcLYTPP4ePPiIpKwnA8XcJViusXOl8L6Y//ii4XXigWmv48cdSZ0Q56vIXdhnti9nH9NDp5XrMD3Z+QJ+5fcr1mFC5puImnfqTKXveJXJB+X524u9JAuEiqnpV5ecHfubHoT+yYNgCfM2+fDfwO4ICgjiT4nws4KztgnHSv/56AA439KJtnbZ0bdyVAx7nSTHlwY4d8OKLkJ5urE3Ys4dUb+P5mZZMeOwxaNYMS3IiB63R1Cp0XnGMITz5JDz7LEmJxkm71EBYuRL69zdes7h9+6BlS+N24UDYswf+8Q9jMV2h1khxF+0yCg+HneU37fKrvV/xzJpnyu14AP9a9y9+Dfu1YB+rcpIfBJWhhdB1xWDe7QqzkzdXdFXENUAC4TLVq1qPjaM30v267rSu1brEz8+mniXNC2b8ZxBpdapz1JTEjbVvpOt1XbGh2dkIGDcO3n8f/P2Nje+0JrVmFQAystPQ9o309jzWjzQPC0ML9U5lmCnS7ZS41dgCKjEr0XmF9+83/j5xosjDOieHFdkHsdxr/4Zc+MRfeOD7pZfgoYectjAuOsvo+uuNa1SXk6SsJLLyslyyPXj+Gg8Aq83K2ZSLt4zKUlnGEFJzUjmSbrQwYzMrz/oVUXlJIFyBBv4N2DxmM491eAwAH08fFIpA30A+3f0pwV8F8+jJ/zF19iPk2HJpW6ctnYM6Y/Yws7WVj6M7qLCUKsaWFDYPyDXB2QCYUNvYOvsf1jaOchleFOnvT9pmzBBKSjrntK5bwjdh8cD4xl7Iri1zGfBAHv9qctLYWqNwC+FcoWPNng3z58PGjSWOXWoLIS6u4HZW+Vw8J78FlJxV6CJDFssVz5ACaFytMQBbzxRcR3v+ofnc8OkNpbe4LkF+UFb0LKPTyQX/z6Ksf7+LM4nyJ4FwFRoGNATg7ZC3mTVoFsH1grmQfQGlFDV8ajBj3wwA2tZui5/Zj/Z127Onk/Ecnn/eWODWpw8Z/XqTZEkh0GzsB5hphqk94XAd6GxuSrcPFrJyHoz5AzJ8C+1l9N13jhNX4t5txmA2wGuvoe+4nQMLP6VHsy282Q0+TFqFTdscT036fQsAK61HoUEDoyVx5gyHzx8m+VyYsX+TZ6HX+vTTEu8/v2vEjIn0YwXTYy8smktYDfudiAjnH158PDz1lPEZFLP1zFZCo0OLPOYIhOxCJ7bnnjNmSO3e7fw1ypAfaHujCy5idCzhGDnWHKLTrnzxX/5xLTYLudYK29md8GTjS0CddIgyVfx4RmWwL2Yf5zP+mtaS1prxy8dfUyvWJRCuQs8mPXm4/cOMvXksDwc/zM8P/EzohFAOTTxE9ybdSctNo4q5Cm3rtAWgU4NObFWR/OvdEOKfn8SmFl68MeUOTnz9DgAdGt8GQMbQ/hxv5EuDVFjZdy60aMF9XcdSp91tZPiaoHt36NoVxowhqaNx7CRfYNYsyM5m+LE36dp6FzsXfQzAf3rAc3X382tYwbfpmMNG6yMsNQJduxbs3o2taRPu/O5OnrOuYsAoM5GtGxS82bVrjavJxcaCzUZKdgrrw421GfVTbCQcNU7gB2IPUCPheVo9CTkmnLaGAGNq7qefwtSpJX40adUkJq+dDBgL87rN7OZYoFfkm3v+NSqczdwqzd698Oij2GJjHMcqfILID4KruRDRZQ+2nzgBW7eWXe4i/oj5o8SlTPMvXNQ1EqKqWI0xKzfXe05v3tn2zl/yWheyL/DtH9+y4viKsgsX8ujyR1lydImLanVxEghXIdAvkNmDZzsuslPdpzq31L8FX7OvY3yhf8v++Jn9AOjYoCMWm4UPsjfy8aEZfPL7J7y6eSrbzhrbX3So3wGAzE8+4FRNuDfaj8DgzsY39W+/xe+uvuRYc7Bu3GAsgFOKpHbNAThdA+7wnEV6NV8WtLayozF8UqdoN9FX306Eu+6CuDhi4gpWO4flxPLC3dDtEWPX1Xn+EaxolMmGtka9efBByM5m0ke9WNulPkybxtMLx7Lq5CoAOp/V7PRLRFutLD9ubBCYZ4LwGhQEQk4OfPON0c1jsxUMOO/fb1zDupCzqWc5GHcQrTWbTm9iW+Q2x0k26Zx9+uTJkwUbBe7dC4mljKEUFh9vXBVvxgxSXn7W0WKKz7RfZyM7m+iT+wBjfceVKq0rbd7BeczaP6vkE1q2NEL+CiVnJXPrjFsdLdJ8p5NPU81mpl0cxFWF3Ii/doX7qhOrePKXJ//S17yYjNwMErMSic2ILbtwOcj/UhGXEVdGyQI2beO7/d+x9PgVXn/lKkkguMjD7R+mXZ12vBXyluOxjg06Om6HxoSyK8r4lv7V3q/wUB4E1w0GIDY9lvMeWTQfPRk8Cv6JqngZA8+Zlkx2nfud387+VuQb82+NYE+hL/XHa4GnfVfqTudglTmC7K0b4Z//JLZKQffRwWceYvmt1dhhdKmT62EMIJ+uZ0x9ynxkJLF1/Pgy7zd+aAfMm8fJI9scz+8ZAef8Naf2b+RwWMElQ4/X8yzoMpo3DyZMMC44FBFhhMD99xtTbPP3g0pIIP3l50nNSSUlJ4XImKOcTS06wJv09v8ZN/bYF+z5+sIXX0CbNqXuJFvwRg8a25l36EDisvkABAUEkZCZYKz0fvpposOMQfiENOOXOM+WR9eZXVl9cvXFj11I4bGDjLnfQY6xkvzF9S/y3o73+PPsXlLvH2C0kNq0KfTE0rt1vt77NTsiC80Ui4mBVUYgn0k5Q54tr8SlTMMvhNM0y5dGxppHBq0f/5des/uHQz/w+Z7PycnL+cte82LiFs4Eio1DuVD+F43L6aJKykrCpm2cS3U+JuhqEggu0rp2aw5OPEizGs0cj7Wt05ZRwaO4reFtrA1b6/jmcDThKM1qNKOGr9HxfjDO6I9v3vjmIsesYrbPRLJkcPu3t3PHd3eUGPxc859RAIxu9SAAb5+5nqOD1vPCLhN5JjhSG1i4kJgWDRyDqqEBGZwwp5Z4DxHVjavGNTswlvqTjG+9x2sBoaFcSCu4el2PCOPvFit7s+DsarrZ759o4g8ffGBMsc2/4tzKlcaJGYgd9yAZVcywYYPxs8mTiZ7xkeO4B16fxNmkoie55IQoY9D64EFjnKO9scUI58/Dxx+XeA9FnDxJZDW444FU9tczHmoZ2JI8Wx4pox+E6dM5V9MYN0kINbpwzqacZXvkdu794V7umXsP0dHHiV8yr9QV51CshfD267B6NYfjD3Mu7RzhyeHc+v2d/DdxhTGGcrTQFLJSutfybHk8vupxuszsUvDg5MnQrx+89RaRKcaGibHpRb/5nk4+TbNUEw1rG/8HV6fsdWzF4szy48t5ddOrzn949qxxjfEyaK1Ztn8+ue+/w8kEY2bbubSKObkVFp0Wzbb//hO4yBRtu1/DfkW9roi4EHHRclmWrFKvXwIFLYTLCYT4DOP3qqI+MwmEv5CnhyezBs3inbsK+jCbVG8CGHsn5Z/w/4gxFozdEHhDkefnB8a8g/Mcj0WmRBa5aM4v6UaXxyO3TSS6x0omf7qXVsF3cdMSY7fU/b3bM31YU5ZWi+GGmjfQKKARPx35CU3JaaWn63mz7usXiMsqOPkfv64qeUpzsiY8XrUnp2bXoGW9tgw/XnAluTuqtqJulbqc6NQMbrzRmGK7YgUoZbQUBg/G4gH1dw1j2Fh/YybT+PEwdy7RDxRsEHjw2BbO/rqoSJ2SfDFWge/bxx+dm3DbkETO39AAevUywicpCQ4dgi5dsP20oOgv7MmTrGnlyW9ZJ5lu9M7Ryj5UEL9qITkmSPQytvNO3LEe3n2Xs9tWOp6+NmwtDb9pRZ2DI9H9+xkPbtpUom++yBiCF3D0KGtPGdODc6w5ZNtyHSvYiyg2E8zx71BotlD+yd8xjfj99zmbHAFATHqMMT04MxOtNacvnKZpopVO9TrQIF0BEJVa+or0gfMH8sbWN5yXef55GDiw1Ofm231uN4OWPcTiOS9zKvZI0TpforWn1vLe9vdKPJ6UlcT2yIvsLnwRTy99jDGDC45zMTP3Gy2JzRGbL1pu5JKR9P+xf6k/v5Iuo/xWxdVMargaEggVoGfTnswZPIeR7Ueye/xu1o5cy5f3fUmLwBZ4m7z58dCPmD3MXF/j+iLPG9ByAK1rteb5dQWXlkjLTaOGTw3H/fzdWZvVaEb97vfhGVDduN+sA1W9qrJzcEceu/E0Gk19//q0rt3aMWCrUHiZvBzH2h67m3tj/1ekDhes6ez6/k0sJujc82GuX7EdNX8BPz73G0PjjY2bGg8cRYvAFpxo5GeMEbzyCn8O6Mzod28j/pZWAGy8qykAawKTjG/83xqXLz13RzsAvExeHGxZnbNeRbs4knwxFuWtW8fXHTS7s06xcO7L8OGHRpfR++8bXVE7dvDlB8O57u06BV0WJ0+yr4U/AOvsH23LBUbrJMEPYloHOV4nIfcCTJlC5NSnnf4bJkWeMMItJMQ4WYJjrUbm9oILBGYE+sOxY6wNW1vk4kYnAzG6u8aMgWr2q82W0kI4HF8waN5hegf2nPnNGEhv3BhSU4k8acySikmLMT6HevWIjThEdl42Tc+mU7PB9RzYbSTgxb555o91LTqyqOQP7bPQyppGvC/G+EKyKwiStRHGZ8JCL/aUEr7Z9w2vb3m9xIaNH/72IT1n9SwxeA4YoXyRa5YfiznkuF1WIOR/McvfcLI0h84fYuPpjZy54HyRauEWwqVuPpnfQkjNSa2QdSwSCBVkZPuRzBk8h9pVatP7+t7UqVKH2lVq81iHx8jKy2JKlymOMYN8fmY/lg5fyoCWA7inecE36W/6f8NX933F/W3udzzWwL9Bkefmj1F8Z//2A8Z/vibVmjjKD249mDHBY/D38qd5TWOwukn1Jqx4qOgsiWeyjBkQreu2NfrA27aFDh2Y9s5+BrYcyJDOY2lVqxWH4w+jPTxIeuVZOnX6g9lZu/jif8PRGRnMHtvBcTyLB1ja3wgvvkh0IyPcujbuyoEba3G2VtFrWM+8zYudo3piVbC4urGYbtHRRdCuHbbedxsnxGPH4Mcf2dStEQlkcHTZDGPsYsUKQusX/cVsZTUC847xcO/Agl/AxIG9ITaWyFZFP8d84TUgefxIck3A4sUwaZJx3exBg8g4G469oUH6DY3JPHGYrWe2cl+zgn+zU7U8WLlvPo8O9uTTtf9hb1NveOYZWL6c349tIO/9dyHVOCEdOW8Ewvzak1AoXlv9gjH2MtmYiXU2wt4Fd+64EU5paZxe9A0AzWwB8PTTBDZvh3deQQvho98+KjGTJf/LwPI9c1l2dEnBt9SsrIK9kEppxXD8OJw+zf5YYwxmbaHvMpFzPzdu2GxOnlhSxLnDZOVlkZRRdKbX4fjD5NnyiPzvKyWf9NZbcMstTseRtNaEZxUE4YXsC8aEgqgoGDLEmDQBsHlzkfdX+ET/9Oqn2XS6IOi11o7P8qfDPzl9H/kn9+y87Es+uTsmOABrTq3h9m9vLzEF25UkECqZN0Le4NsB3/JKNyf/6YEWgS1YNnwZq0esJtA3kDa12zCg5QAe6/gYdze7GzBmK5k8TCWe26NJjyJdQ8PaDOOe5vcQ4B3Azw/8zM8P/MzX/b8m6tkoJtwyAYBxN4+j+3XGDJj8lkhoTCgPt3+4yCA5GKGydPhS6laty831biYpK4nIlEiWHVtGjjWHAO8Avtn3DRPWP80PxxdRp4rRbzJjzjP4Dj3K2/cFcC4jhqpeVenSuAsnkk+RhaXIa2TpXO5stomf5r5EPBkE1w1m65mtxKbHcmfXEzx6jwX8/GDQIPY3MsJk/0cvwrBh5JrgoG8a3a7rBoBXHjSfV7AO4qj5Ah3qd+CmejdxLOk4YeZ0zva9w/Hz0YfNDEwz1pGcqgk3ToJ/j29Gclo8k8O/5OGRfmzZv4xMM9S19xr91tjEloRQcqw5TPhkOx72X7kcDxv9FwxkzsE5PLXmabqMyCGsBhweP5DOC3oxefMUY43FypUcCV1D4wvw4L++55+tRvFL3HZe7AX9/Jaib2xLZKJxEkvxgayaAXDDDYSvNQbNm/Z+EOrVQ7VqTcNUiEoIx2rJ5f82/R+vb3nd8d5SslO4kH3B+PeNDmXwgiG8u/1dft4zm7C9vzLnRvvJvPC1OPLy4M8/+TP2IJvGhcCoUew/a6wJOWbf5VcBkYmnYdky/rzOh9y5TmZZFXMmwQifyNeewWqz8p8t/+Fc6jmOxxldUKdnfsTXOz/ls92fFazz2LnTCK4tW3h85eN0mN7B0eUTnxlPZqH/RxpNZEokea+9CkuWwE8/waFD7B7Zk/h21xMXbXTHhSUb7zUu6hjTdk9j9sHZBZ9XToqjO3L+4flO30dCVkGgXWq3UX6IALy0/iV2Re0iZHbIRccqypMEQiUT4B3A2JvHFum6Kc2pp06xb8I+lDL6hx/t8ChJLySx7ZFtTsuPDh7tuG35PwvjbxnP4NaDufDiBToHdS5ShwkdJvDfu//Lc3c8h7+3P9fXuJ6QpiH8/MDP7Hl0D7MHz77o9Z07NDBaAFM3T2XiqokEBQSx5MElxGfGM+OPGbx050tsHGWsfn729FdYtZVXNr7C0mNLCQoIcsy4goJxFrNHQWthcvRMOtTvwLS+07BpG/1+6MeuvAhmdIAx42ux43woYReME+XEnhnM6FWT0O/fIVdZebLTk8Q/GcnhYRup0/RGxzFjn4tl74S9tAhsQXhyOM0/bc5XpxfS2KMGwbHw7G8w9wHjl3/FyE5EB8DPrWwsnNSdj2+HhY3SuGckRPtDi6rXMbR2d6b5HeS5PuCbp+gVV4UbEmxcd6Hgc0p+MZnDkw7jZfbhqQlB/NbIePyz28D6+y4sA/uzJfo3gtOrgM3G498dxCdP8X4XWBWzha9GteGwT5pjNlnMqT/gqac4bTFOLE1us29N0qULQalwbstKTjT1J8OSwYG4A8R8+DqcOMGZz94EjNloqT6gFXy950uG/TKa5hsHMWoInK4OnDrFvph9xjfexx+H9u15/LN7GNQ1mozft/Nn9H7He/P38qd9rRsJq+VB1KhB3DzWwrvLXzC+nWttBArAzp2s27eQjKRY0nPTSbA3ryI3LmV35E6mbp7KAwsf4FSiMZC/uQk8vu4p/rn6n3w0Y5zRXbTP6KpKX7eKr0O/Zl/MPsc03PCwgoWH+Zp+0pTAOjM53ckYp8t4+Xm6PWIsCI09awz0hyWFQXY2+3sYU8iPbFviWASZPwvotoa3sS9mHycSjpd4jcJrWc6nRBtTxctYRBmfVDCrLiw5jADvAFJzUv+yxW0SCNew6j7V8fb0LvJYDd8a+Jp9nZa/IfAGul/XnUdvebTIQHR+oBRWzacaz9/xvCOYlg5fyod9PmRI6yElWgbOtKtjjAXMOjCLHGsOg1oOIqRpCCf/eZLNozfzTq93aFWrFU2qNyE7L5uQpiHUqVKHMylnmHDLBMcFigCOTDriOFm/3+t9wOiX/eK+L7g96HbAaLXce8O99De15ufaCUVm5GR7wqNdklhcJxGFIqRpCLUCG9E8uGeRbrm6VesCJaclelevxf5/Hqb97jNUvbULtf1q82O2Me01PCWCuTfaqOlbk62PbCXbDEfqQJU2wfz42DqqeQVwtDZ0bxaCz87drEkdwLrmxjfzz/p+hq/Zlza12/D63W/zi28ULw32d7zukrdG8lNbiKpq5bGbxsHbb1N76TpG7y9o5U3KXIivzYOJB41/p2m/TyOzXx92N4QGqeBze1ej4O23E1S3OVHeOYQOvNXx/Hk/v8YLT7Zk/eIPAOhTveDfNlfnFfkcFt/szYbTG+kwvQNvvXcffPstCc3q8ZtnDKk+8MLdkGWGYPtkp5CmIXRp2p1N19lo94TC6gFT256n/r99Od6mLumB/lwY+w+ODe5K7xUPMOzp+pyZ+A/H6501Z7J750IAdkbtxGKfDv3JbWDSCn+LBzu2zMX60HCWN0xnyY0m9u8xujermKuw/sQaTiWcIPyXeTiT6g3fPX4r0f7ww7m15HjC7rbViLV/sw+PPYLt02mOGWlHvFLQ48dBnz5EnTNCY3KL0SgNC94ZCUDEhQi+Cf0Gq81KQkw4te0txb1PDoEePeCOOwpaM8DxhOP0nNXT0T0Xv2sjQSkQZN9vcebAmZiUqUh3lSupK77SVgXo2LGj3ru3ZNqLykm9bgTN9wO/p3/L/tT0rVmizOKjixn601C+HfAtVcxVWHhkIT8O/RFPD0/e2f4OQ1sPpWWtlo7yebY8mnzchIEtB/L5fUbfdJ3/1iE+M56Djx+kXd12xKbH8tyvzxEaHUqvZr34fM/njuff2vBWfh//e5E6tP2iLd2v684X930BGLO81oWv45GbHqHOB3WY2n0qr/V4zVG+84zO/H7ud6p5VyMlx/jN7X19b9aMWEPQR0FEp0XzXq/3eOHOFxi+aDgLDi/g4z4f83TnggFqq81apFsvz5ZHx+kdORB3gJvr3UxWXhbhyeH4ZFu57oJm/zvJeFSpCsuXk2TO49fGFh5abJw89wz7lRrV69N8hhHCvp6+ZOVlMXEPfLGy4Pf7hV//xSe/T2PszWOZdWAWjRMsHK9e9KS/7ZFtdJ3Z1em/541pviTrLM4FQKt4eOVEXR6+0+gK8bIpx/qV/2yEV0Pgw94f8sStTzB++XjmHJxT5FhtcgJIMOXgn5rDuH3wci/j8S9WwiT7BK425yHHx5OwgKJ1BBh63IR3jpXNTaDbGZhvvHWe2gXTOsPrh+swta0x3fO28178XieXPQ9u4PDwuxyzjXql1mJ9QAIBFhOpZqOJ5YkHedhonginAmHmUljTHBbYG5KRH0JQKsx8bQDjWE74xzBmECT6wf4HNnJ/zDSWHl/KgBv6s+fAL9x52krkddXYXTWF1QGTCPjsG1rFWPDseRcDH7ByMvEk59LOoVD0vr43mVs3YKlRjR1fW0h9cBDVv57FHR+2Raem8NurkUXWJV0qpVSo1rrsb3FIIAgX2n1uNzZtK9Id5czR+KO0rNXyol1QhWVaMvHx9HGUD40O5c/zfzLmpjFOy1ttVrp9342dZ3fyZs83Sx2fcSbXmovZw1ykFTV101Te2PoGT932FPtj97PlzBaeu/05Puj9Aftj9xOXHkef5kZXzZKjS3hw0YMceeKIY6C+NAfjDhL8VTCv93idm+rdxKglo7jBrxGL27xGo15DS5T/747/cjzxODMGGF0jkSmRbAjfwILDC3i6yYPcU+NWVNu2jvK/nPyF+364D4C7m93Nh3e9z30/DWTCLROIuBCBVVv5qt9XVHm7ClXMVUjJScHTw5M8Wx4DWw5k2fFlmK0wMrEhM+sUDNLe3exuxt08luE/P8To64fyWcdX+SJhNc/cPhkvkxcnE0/S4rMWNPBvwKgaPQlVMaw7U3SzRE8baA8PPKw2LMWGv0LCoVM0HOnaiuRGtdgeuZ3NM2F3m2q8cJsRyI/7dOGrbGNKar002DsdOv3TmxgvY4ZZz6rt2PjsAY7U9aDtE8Zxf71vAZN+e4XGXnXYGFu0S+ajW17mp99mcJR4lH8A3smpxFYxzpUB2dDvBPzQHrJnBvHt8BY84VvwfjqmBbDPPw1l0yxs+gJ9Rkyl5WctHYPQQWmKgUc1n9+KUw9438KCnQ2NBZtPP80re95jSQsrB1+Lc8wavBwSCEIUY7FaOJd2jqCAoCLdZVdKa41SimXHljFowSAW3r+QYW2GOS2bnJXsWENSlpi0GGr51cJsMpdd+AosOrKI36N+56UuLxHoF+h4H4XdO+9eOgd1poF/AxIyE5iyYQrHnzxOaHQoPsqTjkG30ebLtnRs0JH5Q+c7utrOXDhDLb9aJWbHgTETJ7huMC1rtSQ2PZZlx5bxcPDDvLf9Pf6z9T882/lZTiWfYvnx5dxYpSkeVf05GHeQIemNmNh6FL0OpMHUqfzv2EzWh6/nl93NWX+jH71jjC5E26s2hv40lCXHljDikAdzq49Ff/452797jfcSlvLtUxuoG1Cf6B++puHJx6lirkL6ywUzf7ZEbCHTksm9P9wLwIJhC7i1QSdCZoWQbc1h2b2zGfTLaJqrmmxNM6aw1jNVJ+bfySRkJtDjo2A8o6I5Xlvx5+eamKpgqt+AO3ZEgsnEiuMreHH9i4wOHs1LG14CoMdpCG2o+Hi1Jtof9jaAQ3Vg3eAlNA0Nc0xnzmvUEM8Nm+CGouuSLtXlBAJa62vmT4cOHbQQlc2JhBPaZrNVdDVcItuSrbdEbCnxeHm+37CkMJ2Zm6n3x+zXvef01mFJYTouPU5Hp0Zf9HlpOWn6nrn36D3n9jjuH40/qvMyM7S2Wp0+x2qz6keXP6r3Re9z+vNlx0EgO94AAAewSURBVJbpx1Y8phMzE7XWWqfnpOsLWReKlJm9f7Zu90U7vfL4yoIHbTZtCw3VaVkpWnfpojVovaXk56a11lPWT9HPfPeAzj51zPgcU1O1XrVK5yl0nkLrrCytDx0yjvH++1pf5WcN7NWXeI6VFoIQQpSn1FRISYFGjS7veTfeaGwCmb8tSlKScQGtq3Q5LYSrbzsLIYQoEBBg/LlcoaFFF++VQxhcLgkEIYSoDLy9yy7jYrIOQQghBCCBIIQQwk4CQQghBOCiQFBK+SilViqlDiil5igneyNcShkhhBB/HVe1EEYCUVrrYKAGcPcVlhFCCPEXcVUghADr7Lc3Aj2vsIwQQoi/iKsCIRCw79dHKuBsQu2llEEpNUEptVcptTc+Pt5ZESGEEOXAVesQEgD7dQGpZr9/JWXQ+v/bO/MQu8ozDj+/JJMYl6LGukUtKLiAJlYaazW1aheDjhpN3RAxBsU/LChqjFtpaS2KxNZSNRg14opEE8EFUzUIkqAGFwxRXHCpVYlrTbRxz+sf3xeZ3HvGjMF773c8vweGuXPnmzPP+XHnvuf7zpz3xGxgNoCkdyVV369u3Wwx2O8okDq5gn07SZ1coV6+dXKF9ff9yVAHdqogLAR+B8wjLQ39Yz3HrEVE/Hh9hSQ9MdTLt3tNnVzBvp2kTq5QL986uUJ3fDu1ZHQrMFbSUuAD4GVJM9cxZmGHXIwxxgyBjswQIuIzoL/l6XOGMMYYY0yPaNKFabN7LfAdqJMr2LeT1MkV6uVbJ1fogm+t2l8bY4zpHE2aIRhjjPkWfvAFoQ4tMiRNkvSGpEX5Y3yJzpL6JN2TH7flWlrWLb6tGe9Sim/O7kZJj0m6W9LGJWdb4dtfcLYjJN0habGkOaW/bit8u/q6/cEXBOrTImNWREyMiInABApzljQaeHKAS1WuxWRd4QsDMo6IFyjHdz9gRETsA/wImFbhVYortPuuptxsJwPPRMR+wDbAHyq8SnGFdt896WK2TSgIdWmRMUXSEknzgF9TmHNEfBIR44A38lNVuRaTdYUvDMg4H1WV4vs28M/8+HPgzxScLe2+UG62C4C/SxoBbArsVeFViiu0+66ki9k2oSAMqUVGj3kZ+GNE7E06KjiK8p2rci0569aMf0UhvhHxUkQskXQkMJI0syk22wrfkrP9OCJWAYtJhazo122F74N0MdsmFIQhtcjoMR8AD+XHr5Gm4KU7V+VactatGW9JQb6SDgfOAA4D3qnwKsYV2nzfo9BsJY2RNArYl7S8snuFVxGuUOk7ji5m24SCsKZFBqSp1sM9dBmMs4DjJA0jvWDPpnznqlxLzro142UU4itpa2A6cGhEfDSIVxGuUOlbbLakv6WjI+IrYBXwtwqvUlyh3fciuphtEwpCHVpkXAmcDDwO3AVcT/nOVbmWnPVaGUfEc5TjexJpOeDfkhYBfRVepbhCu+8qys32KmCapEeB96n+2yrFFdp9++litr4wzRhjDNCMGYIxxpgh4IJgjDEGcEEwxhiTcUEwxhgDuCAYs6Z/zBhJm0uqzR20jPm+cUEwjUXSXEk7AT8DbgHGkvoItY67WNIRkkZJuic3F/vFt2z3WElTBvne+ZImSrpE0gxJm0m6SdLY72u/jFlfXBBMk3kKOAQ4AtgZuAY4VNJDkv4CqbMncDjwSL7L3y7AKGCOpB3ymA0kPSBpeN7uCcDS1l+Wt/UkqSHcF6T/318J7AFsI2lCvujLmJ7QkVtoGlMTLouI1ZKuA35KerOfApxP6tED6cKgVyPif/nrVRGxQtJvSW0EXo+ITyU9Q7qgaAHpitJZuStxH7AjsEP+PB34ObAc+C+p9cCHpMJ0POmir+Wd3W1jqnFBMI1E0iRghqQ7SIXgTlKTsE1ILYdHSzoV+CvwvKRxpKWlrSTNJx3hL5P0dKSrOy8HdgL+BEwFNgDeIi1D9UdESPoP6arSt0lv+rsB2+bfPRfYMSKWdGP/janCBcE0kohYIOlTYEJE/BIgzxQ2iojj89dnkRqLbUeaQawgLTPNiIiXWra3XNKXwA0RsVjSJcAjwLPAi3nYGFIvmpHAvaQmhgeTZhSPAm92cJeNWScuCKbpBICk8aTOkvdLujQizgNmkZZ0roiIG/O4rYDfAGsVBEljgHvzTWMgnZN4JSLuHzBsCnAiafnoANLyU38uRJOBpzuzi8YMDRcE03gk7QvMAX5POqKfL+mciJgpaaOW4fOBeZKuAYiI1fn584BL8/a2A/4PHCVpbkSsyGOvzjc+2ZP05r8y/+ytwO2kcwzG9AwXBNNkDiIdqQ8HpkbEMgBJ00gnfgGUP4BvloYWAlcAwyQ9ALwA7B8R03MxuBk4FdieNOM4PSLWHP3/CziXdJ7hS0kPA6eTlosOJC0lGdMT3O3UNBJJ/cBppHbDp5DOE0A6SNqQdHOSY4BXgSsjYvKAnx0OXEu6Ocz4iHgrLxltT5ppTI2IpXns/qTW25NIS0UXAouAmcCuwG3ABaRzCPcBZ0bE453bc2MGxwXBNBZJIyPi83WP/E7b7IuIL1qeU/4voz6Agd+XNDoiPsmPhw1YgjKm67ggGGOMAXylsjHGmIwLgjHGGMAFwRhjTMYFwRhjDOCCYIwxJvM1YTnm/uY4e6EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc1c1be0b8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = LSTM(X_train.shape[1],3,64,2) #512,2 0.983  256,2 0.983 ,64 ,2 0.984,32,2,0.983,256,4.0.983\n",
    "\n",
    "if train_on_gpu:\n",
    "    net.cuda()\n",
    "from time import time\n",
    "start = time()\n",
    "epochs =350\n",
    "accuracy = train(net,epochs,train_loader,valid_loader,clip=5,lr=0.00128)\n",
    "print('Training time is:',time()-start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9676a792cb1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.xlabel('Epoches')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#plt.ylabel('Accuracy')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'准确率'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'准确率变化'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'迭代次数'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "#plt.plot(accuracy,color='b',label='accuracy')\n",
    "#plt.title('Accuracy_Trend')\n",
    "#plt.xlabel('Epoches')\n",
    "#plt.ylabel('Accuracy')\n",
    "plt.plot(accuracy,color='b',label='准确率')\n",
    "plt.title('准确率变化')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('准确率')\n",
    "plt.legend()\n",
    "plt.savefig('behavior_image/accuracy_lstm_behavior_modefy1.svg',dpi=300)\n",
    "plt.savefig('behavior_image/accuracy_lstm_behavior_modefy1.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.025245\n",
      "\n",
      "Test Accuracy of left:100.0000(42/42)\n",
      "Test Accuracy of keep:95.9184(47/49)\n",
      "Test Accuracy of right:100.0000(34/34)\n",
      "Test Accuracy(Overall):98.4000 (123/125)\n",
      "Test loss: 0.034 Test Accuracy:0.9839907884597778\n"
     ]
    }
   ],
   "source": [
    "net_test = LSTM(X_train.shape[1],3,64,2)\n",
    "net_test.load_state_dict(torch.load('model/lstm_behavior_prediction_modefy.pt'))\n",
    "if train_on_gpu:\n",
    "    net_test.cuda()\n",
    "\n",
    "test(net_test,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 3.4987926483154297\n",
      "tensor([[-57.0345,   6.1797,  51.5773],\n",
      "        [-57.0345,   6.1797,  51.5773],\n",
      "        [-57.0345,   6.1797,  51.5773],\n",
      "        ...,\n",
      "        [-57.0345,   6.1797,  51.5773],\n",
      "        [-57.0345,   6.1797,  51.5773],\n",
      "        [-57.0345,   6.1797,  51.5773]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "net_test.cuda().eval()\n",
    "h = net_test.init_hidden(5000)\n",
    "h = tuple([each.data for each in h])\n",
    "example = torch.ones(5000,1,4).cuda()\n",
    "t = time()\n",
    "out,h = net_test(example,h)\n",
    "print('time',((time()-t)*1000))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
